[
  {
    "objectID": "code-projects/christmas-movies-2022/index.html",
    "href": "code-projects/christmas-movies-2022/index.html",
    "title": "Christmas Movies",
    "section": "",
    "text": "Link to code on GitHub.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tm)\nlibrary(dplyr)\nlibrary(wordcloud)\nlibrary(plotly)\n#library(rio)\n\n###data source\n#url <- 'https://www.kaggle.com/datasets/jonbown/christmas-movies?select=christmas_movies.csv'\n\n\n#url <- rio::import(file = url,which = 1)\n#df <- url\n\ndf <- read.csv('christmas_movies.csv')\n\n#clean data\ndf <- df %>% filter(!is.na(description))\ndf$release_year <- as.numeric(df$release_year)\ndescription_vector <- as.character(df$description)\n\n#df\n\n\n\n\nShow the code\n#remove punctuation and numbers from description column\ndescription_vector <- gsub(\"[[:punct::]]\", \"\", description_vector)\ndescription_vector <- gsub(\"[[:digit:]]\", \"\", description_vector)\n\n\n#remove pnctuation\ndescription_vector <- gsub(\"[!.!»?,-]\",\"\", description_vector)\n\n#create a Corpus object\ndescription_corpus <- Corpus(VectorSource(description_vector))\n\n#create tokens from individual words and phrases in description colum n\n#make everything lowercase\ndescription_corpus <- tm_map(description_corpus, content_transformer(tolower))\n\n#remove stopwords\ndescription_corpus <- tm_map(description_corpus, removeWords, stopwords(\"english\"))\n\n\n#remove a list of words\ndescription_corpus <- tm_map(description_corpus, removeWords, c(\"the\", \"an\", \"a\", \"must\", \"and\", \"could\", \"would\", \"have\", \"set\", \"exists\", \"turn\", \"tale\", \"might\", \"else\", \"causing\", \"whose\", \"whos\", \"sets\",\"put\", \"...\", \"always\", \"gets\", \"before\",\"very\", \"loosely\",\"with\",\"various\",\"very\",\"into\",\"bearing\", \"behind\", \"predictably\", \"follows\", \"eight\",\"frantic\", \"tales\", \"lives\", \"dealing\",\"eightyearold\", \"inadvertently\", \"finds\", \"thing\", \"take\", \"taken\", \"tries\", \"several\", \"kevin\", \"mccallister\", \"become\", \"commanding\", \"like\", \"three\", \"former\", \"general\", \"cynical\", \"griswold\", \"walter\", \"desparately\", \"frustrated\", \"never\", \"sent\", \"unfortunately\", \"becoming\", \"left\", \"becomes\", \"really\", \"begins\", \"marie\", \"miranda\", \"four\", \"seemingly\", \"meaning\", \"emily\", \"something\", \"also\", \"actually\", \"early\", \"despises\", \"thinks\", \"wagner\", \"jill\", \"mark\", \"appear\", \"reluctantly\", \"decided\", \"fending\", \"indulge\", \"wants\", \"matt\", \"meet\", \"comes\", \"going\", \"best\", \"think\", \"leaving\", \"thought\", \"approaching\", \"still\", \"heading\", \"picked\", \"decides\", \"seven\", \"shows\", \"learns\", \"lucas\", \"agrees\", \"happens\", \"robin\", \"mary\", \"jacob\", \"laurel\", \"facing\", \"quickly\", \"shocking\", \"peter\", \"mariah\", \"everett\", \"sebastian\", \"roped\", \"austin\", \"jokingly\", \"timothy\", \"emmanuel\", \"attends\", \"carlton\", \"mcandrick\", \"sarah\", \"palmer\", \"christmas\", \"christmastime\", \"summary\", \"»\",\" »\", \"«\", \" «\",\"tasked\", \"helps\", \"things\", \"make\", \"keep\", \"falls\", \"makes\", \"another\", \"will\", \"come\", \"spend\", \"meet\", \"meets\", \"goes\", \"bring\", \"full\", \"takes\", \"just\", \"decide\", \"unexpectedly\", \"help\", \"show\", \"years\", \"find\", \"back\", \"york\", \"plot\", \"claus\", \"holiday\", \"young\", \"season\", \"time\", \"year\", \"small\", \"little\", \"girl\", \"woman\", \"man\", \"true\", \"plan\", \"plans\", \"lizzie\", \"virginia\", \"richfield\", \"pull\", \"begin\", \"foxworth\", \"maggie\", \"harper\",\"whole\", \"current\", \"starts\", \"name\", \"zeus\", \"amalie\", \"hess\", \"brings\", \"receives\",\"need\", \"pole\", \"place\", \"others\", \"plaza\",\"existed\",\"desparately\",\"bumbling\", \"vermont\", \"adam\", \"calvin\",\"langton\", \"anymore\", \"expect\", \"massey\", \"kyla\", \"cordinia\",\"isadora\", \"leopold\", \"jonna\", \"nick\", \"julie\", \"walshrick\", \"mgtow\", \"briana\", \"laura\", \"kylie\", \"chloe\", \"evan\", \"third\",\"noelle\",\"lets\", \"serving\", \"even\", \"anna\", \"galwick\", \"chronicles\", \"story\", \"reel\"))\n\n#remove white space\ndescription_corpus <- tm_map(description_corpus, stripWhitespace)\n\n\n\n\nShow the code\n#determining themes\n\nmatrix <- TermDocumentMatrix(description_corpus)\nmatrix1 <- as.matrix(matrix)\nmatrix_df <- as.data.frame(matrix1)\n\nfreq <- head(matrix_df, n = 1000)\n\n\nsorted_matrix_df <- matrix_df[order(freq), ]\n\ntop_100 <- sorted_matrix_df %>%\n  head(100)\n\n#top_100[0]\n\n\n\n\nShow the code\nterm_freq <- data.frame(term = colnames(matrix_df), freq = colSums(matrix_df))\n\ncolors <- ifelse(term_freq$freq > 2, \"#CDD9CC\", \"#BF1111\")\np <- wordcloud(description_corpus, min.freq = 10, colors = colors)\n\n\n\n\n\n\n\nShow the code\nmin_imdb <- min(df$imdb_rating)\nmax_imdb <- max(df$imdb_rating)\n\npalette <- \"#AEDFF2\"\n\np <- ggplot(df, aes(x = title, y = imdb_rating)) +\n  geom_segment(aes(\n                  x = title, \n                  xend = title, \n                  y = 0, \n                  yend = imdb_rating), size = 1, alpha = 0.5, show.legend = FALSE, color = palette) +\n  geom_point(size = 2, color = \"#4AB0D9\", shape = 8) +\n  scale_y_reverse() +\n  labs(title = \"Christmas Movie Ratings\") + \n  labs(subtitle = \"Data Source: Kaggle. Retrieved from: https://www.kaggle.com/datasets/jonbown/christmas-movies. Graphic: Jisell Howe\") +\n  theme(axis.ticks.x = element_blank(),\n          axis.text.x = element_blank(),\n          axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          legend.position=\"none\")\n\np <- ggplotly(p)\n\n\np %>%\n  layout(\n    hoverlabel = list(\n      font = list(size = 15),\n      bgcolor = \"#F2F2F2\",\n      bordercolor = \"#233A59\",\n      borderwidth = 1,\n      image = df$img_src\n    )\n  )\n\n\n\n\n\n\n\nData Source: Kaggle. Retrieved from: https://www.kaggle.com/datasets/jonbown/christmas-movies. Graphic: Jisell Howe"
  },
  {
    "objectID": "code-projects/posit-2022-table-contest/index.html",
    "href": "code-projects/posit-2022-table-contest/index.html",
    "title": "Construction Spending in the United States",
    "section": "",
    "text": "Here is my entry for the Posit 2022 Table Contest. You can read a supplemental blog post about the project here.\nThe Value of Construction Put in Place Survey (VIP) is conducted by the U.S. Census Bureau and provides monthly estimates of the total dollar value of construction work done in the United States. These estimates are released on a certain schedule and cover work done on new structures and improvements to existing structures for both Private and Public Sector construction. Responses are voluntary and are collected with a certain methodology (U.S. Census Bureau, 2022).\nThe seasonally adjusted annual rate tables below show Total Construction as well as Private and Public Sectors separately. Comparisons between previous month and previous year are made, and the “trend” column shows a quick visual of what the last year has looked like for that particular construction classification. Further line charts below the tables show time series visuals for various construction classifications in the same context.\nAccording to the October 2022 report that was released on December 1st, 2022, the largest increases in the past year are evident in the Manufacturing, Water Supply, and Commercial categories. In contrast, the largest decreases in the past year are evident in the Power category. For the latest data release, please refer to the U.S. Census Bureau resources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Construction Spending - All\n    \n    \n      Data presented in millions of dollars. Rounding may affect accuracy when summarizing data.\n    \n  \n  \n    \n      Type_of_Construction1\n      \n        Comparison\n      \n      \n        Percentage Change\n      \n      trend4\n    \n    \n      Oct_20222\n      Oct_2021\n      Ref_Sep_20223\n      Ref_Oct_20213\n    \n  \n  \n    Total Construction\n$1,794,949\n$1,644,332\n-0.3%\n9.2%\n          1.8M\n    Residential\n$896,505\n$825,903\n-0.3%\n8.5%\n          896.5K\n    Nonresidential\n$898,444\n$818,428\n-0.3%\n9.8%\n          898.4K\n    Lodging\n$19,125\n$15,655\n1.3%\n22.2%\n          19.1K\n    Office\n$87,138\n$86,352\n0.3%\n0.9%\n          87.1K\n    Commercial\n$117,411\n$95,950\n-0.4%\n22.4%\n          117.4K\n    Health care\n$52,410\n$48,655\n0.4%\n7.7%\n          52.4K\n    Educational\n$98,218\n$93,380\n0.4%\n5.2%\n          98.2K\n    Religious\n$2,835\n$2,889\n-9.9%\n-1.9%\n          2.8K\n    Public safety\n$11,736\n$10,601\n1.6%\n10.7%\n          11.7K\n    Amusement and recreation\n$28,186\n$25,026\n2.6%\n12.6%\n          28.2K\n    Transportation\n$56,597\n$55,314\n-0.1%\n2.3%\n          56.6K\n    Communication\n$24,849\n$24,777\n0.6%\n0.3%\n          24.8K\n    Power\n$106,758\n$118,677\n1.5%\n-10%\n          106.8K\n    Highway and street\n$114,273\n$102,161\n-0.7%\n11.9%\n          114.3K\n    Sewage and waste disposal\n$32,706\n$28,051\n-1.1%\n16.6%\n          32.7K\n    Water supply\n$25,470\n$19,140\n-0.7%\n33.1%\n          25.5K\n    Conservation and development\n$9,114\n$8,171\n0.8%\n11.5%\n          9.1K\n    Manufacturing\n$111,618\n$83,632\n-3.3%\n33.5%\n          111.6K\n  \n  \n    \n      Table: Jisell Howe, CDT | Data: U.S Census Bureau, Construction Spending, December 1st, 2022  Additional Information: www.census.gov/construction/c30/meth.html\n    \n  \n  \n    \n      1 Preliminary\n    \n    \n      2 Revised\n    \n    \n      3 Percentage Change referenced with October 2022\n    \n    \n      4 Trends from October 2021 to October 2022\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Construction Spending - Private Sector\n    \n    \n      Data presented in millions of dollars. Rounding may affect accuracy when summarizing data.   Data presented includes the following private construction categories (not shown separately): public safety, highway and street, sewage and waste disposal, water supply, and conservation and development as well as private residential improvements.\n    \n  \n  \n    \n      Type_of_Construction1\n      \n        Comparison\n      \n      \n        Percentage Change\n      \n      trend4\n    \n    \n      Oct_20222\n      Oct_2021\n      Ref_Sep_20223\n      Ref_Oct_20213\n    \n  \n  \n    Total Private Construction1\n$1,420,380\n$1,303,679\n-0.5%\n9%\n          1.4M\n    Residential2\n$887,224\n$816,908\n-0.3%\n8.6%\n          887.2K\n    New single family\n$410,076\n$433,480\n-2.6%\n-5.4%\n          410.1K\n    New multifamily\n$102,593\n$100,988\n0.6%\n1.6%\n          102.6K\n    Nonresidential\n$533,156\n$486,771\n-0.8%\n9.5%\n          533.2K\n    Lodging\n$18,670\n$15,287\n1.6%\n22.1%\n          18.7K\n    Office\n$74,993\n$74,335\n0.2%\n0.9%\n          75.0K\n    Commercial\n$113,366\n$92,632\n-0.4%\n22.4%\n          113.4K\n    Health care\n$40,997\n$38,575\n-0.6%\n6.3%\n          41.0K\n    Educational\n$18,801\n$16,376\n0%\n14.8%\n          18.8K\n    Religious\n$2,835\n$2,882\n-9.9%\n-1.6%\n          2.8K\n    Amusement and recreation\n$14,525\n$12,529\n3.8%\n15.9%\n          14.5K\n    Transportation\n$15,977\n$15,173\n-0.6%\n5.3%\n          16.0K\n    Communication\n$24,723\n$24,551\n0.6%\n0.7%\n          24.7K\n    Power\n$94,686\n$109,370\n-0.8%\n-13.4%\n          94.7K\n    Manufacturing\n$111,118\n$83,201\n-3.2%\n33.6%\n          111.1K\n  \n  \n    \n      Table: Jisell Howe, CDT | Data: U.S Census Bureau, Construction Spending, December 1st, 2022  Additional Information: www.census.gov/construction/c30/meth.html\n    \n  \n  \n    \n      1 Preliminary\n    \n    \n      2 Revised\n    \n    \n      3 Percentage Change referenced with October 2022\n    \n    \n      4 Trends from October 2021 to October 2022\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Construction Spending - Public Sector\n    \n    \n      Data presented in millions of dollars. Rounding may affect accuracy when summarizing data.   Data presented includes the following public construction categories (not shown separately): lodging, religious, communication, and manufacturing.\n    \n  \n  \n    \n      Type_of_Construction1\n      \n        Comparison\n      \n      \n        Percentage Change\n      \n      trend4\n    \n    \n      Oct_20222\n      Oct_2021\n      Ref_Sep_20223\n      Ref_Oct_20213\n    \n  \n  \n    Total Public Construction3\n$374,569\n$340,652\n0.6%\n10%\n          374.6K\n    Residential\n$9,281\n$8,995\n-0.9%\n3.2%\n          9.3K\n    Nonresidential\n$365,289\n$331,657\n0.6%\n10.1%\n          365.3K\n    Office\n$12,145\n$12,017\n0.7%\n1.1%\n          12.1K\n    Commercial\n$4,046\n$3,318\n-1.6%\n21.9%\n          4.0K\n    Health care\n$11,413\n$10,080\n4.2%\n13.2%\n          11.4K\n    Educational\n$79,417\n$77,004\n0.5%\n3.1%\n          79.4K\n    Public safety\n$11,520\n$10,509\n1.8%\n9.6%\n          11.5K\n    Amusement and recreation\n$13,661\n$12,496\n1.3%\n9.3%\n          13.7K\n    Transportation\n$40,619\n$40,140\n0.2%\n1.2%\n          40.6K\n    Power\n$12,072\n$9,307\n22.8%\n29.7%\n          12.1K\n    Highway and street\n$113,428\n$101,753\n-0.8%\n11.5%\n          113.4K\n    Sewage and waste disposal\n$32,050\n$27,504\n-1.1%\n16.5%\n          32.0K\n    Water supply\n$24,830\n$18,466\n-0.2%\n34.5%\n          24.8K\n    Conservation and development\n$9,007\n$8,032\n0.9%\n12.1%\n          9.0K\n  \n  \n    \n      Table: Jisell Howe, CDT | Data: U.S Census Bureau, Construction Spending, December 1st, 2022  Additional Information: www.census.gov/construction/c30/meth.html\n    \n  \n  \n    \n      1 Preliminary\n    \n    \n      2 Revised\n    \n    \n      3 Percentage Change referenced with October 2022\n    \n    \n      4 Trends from October 2021 to October 2022\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe most significant growth percentages and decline percentages over the past year are colored with stronger blue and red lines in each chart. In addition, New Single Family as a category is also noted due to the large amount of spending in that category in general. That said, efforts were made to separate overall category summary data such as “Residential”, “Nonresidential”, and “Total…” . Overall summary categories are not shown in favor of displaying broken down categories within those overall categories in a closer look. Please refer to tables for overall summary and breakdown categories presented together. Please also refer to the U.S. Census Bureau’s definitions for any given category.\nLink to code on GitHub.\nLink to submission on Posit Community.\n----\nSources Cited\nBureau, U. S. C. (2019, April 15). Construction spending. United States Census Bureau. Retrieved December 1, 2022, from https://www.census.gov/construction/c30/c30index.html"
  },
  {
    "objectID": "code-projects/python-r-equivalents-bim-content-revit/index.html",
    "href": "code-projects/python-r-equivalents-bim-content-revit/index.html",
    "title": "Effects of Placed BIM Content Assets on Revit Project Size",
    "section": "",
    "text": "I set out to learn {R} programming language last week. I had previously worked more with {Python}. Rather than jumping immediately into a full learning course this time for {R}, I decided to take on the challenge of creating a near identical result of {Python} visuals in {R}.\n\n\n\n{Python} Output\n\n\n\n\n\n{R} Output\n\n\nLink to code for each on GitHub.\nData Source: ENGworks Global"
  },
  {
    "objectID": "code-projects/tidytuesday-2022-w49/index.html",
    "href": "code-projects/tidytuesday-2022-w49/index.html",
    "title": "TidyTuesday - 2022W49",
    "section": "",
    "text": "Here is my submission for Tidy Tuesday 2022 W49. Link to code on GitHub.\n\n\nShow the code\nremotes::install_github(\"emilhvitfeldt/elevators\")\n\n\nlibrary(elevators)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#df_mfg <- na.omit(elevators)\ndf_mfg <- elevators\ndf_mfg <- filter(df_mfg, manufacturer != \"MISSING\")\n#df_mfg <- filter(na.omit(df_mfg, manufacturer))\n#df_mfg <- filter(na.omit(df_mfg, approval_date))\n#df_mfg <- filter(df_mfg$approval_date != NA)\n#df_mfg <- filter(na.omit(df_mfg, lastper_insp_date))\ndf_mfg <- df_mfg[!(is.na(df_mfg$approval_date)), ]\nelevators <- elevators[!(is.na(elevators$approval_date)), ]\ndf_mfg$travel_distance <- as.numeric(df_mfg$travel_distance)\ndf_mfg$bin <- as.numeric(df_mfg$bin)\ndf_mfg$house_number <- as.numeric(df_mfg$house_number)\ndf_mfg$tax_block <- as.numeric(df_mfg$tax_block)\ndf_mfg$zip_code <- as.numeric(df_mfg$zip_code)\ndf_mfg <- df_mfg[order(df_mfg$manufacturer),]\n\n\ndf_mfg_test <- filter(df_mfg, manufacturer == \"MRL\")\ndf <- elevators[order(elevators$approval_date),]\n\n\n# create plot \npalette <- \"#027368\"\np <- ggplot(df_mfg, aes(x = manufacturer, y = approval_date)) +\n        theme_solarized_2() +\n        geom_segment(aes(\n          x = manufacturer, \n          y = approval_date, \n          xend = manufacturer, \n          yend = lastper_insp_date), size = 1, alpha = 0.5, show.legend = FALSE, color = palette) +\n        geom_point(size =3, alpha = 0.1, color = palette) +\n    labs(title = \"Traveling through Space & Time\",\n         subtitle = \"Popularity of elevators installed and inspected over time throughout New York City\",\n         caption = \"Of the known manufacturers in the source dataset, manufacturers such as A.B. See, Armor, Curtis, Dover, Otis, Republic, Seaberg Inc, Staley Electric, and\\nWestinghouse had some of the oldest elevators approved for install. MRL elevators saw a lot of popularity since the early- to mid-2000s. However, that may\\nbe referring to manufacturers of Machine-Room-Less* elevators in general. Some manufacturers such as Otis saw steady popularity throughout time.\\n| Data Source: NYC Department of Buildings. | Retrieved from: https://emilhvitfeldt.github.io/elevators/ |\\n| Article Source*: https://www.hosting-elevator.com/mrl-elevator/ | #TidyTuesday Week 49 | Graphic: Jisell Howe | \") +\n    theme(axis.ticks.x = element_blank(),\n          axis.text.x = element_text(angle = 45, hjust = 1),\n          axis.title.x = element_blank(),\n          axis.title.y = element_blank(),\n          legend.position=\"none\") +\n    theme(plot.title = element_text(family = \"serif\", \n                                  face = \"bold\",    \n                                  color = \"#262625\",\n                                  size = 20,\n                                  hjust = 0,\n                                  vjust = 1,\n                                  lineheight = 1,\n                                  margin = margin(0, 0, 0, 0)),\n          plot.subtitle = element_text(hjust = 0, size = 9),\n          plot.caption = element_text(hjust = 0, size = 7),\n          plot.caption.position = \"plot\")\np\n\n\n\n\n\n\n\n\nThomas Mock (2022). Tidy Tuesday: A weekly data project aimed at the R ecosystem. https://github.com/rfordatascience/tidytuesday."
  },
  {
    "objectID": "codeprojects.html",
    "href": "codeprojects.html",
    "title": "Code Projects",
    "section": "",
    "text": "Christmas Movies\n\n\n\nrstats\n\n\nplotly\n\n\nwordcloud\n\n\ninteractive\n\n\npersonalproject\n\n\nchristmas\n\n\n\n\n\n\n\nJisell Howe, CDT\n\n\nDec 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday - 2022W49\n\n\n\ntidytuesday\n\n\nrstats\n\n\n\n\n\n\n\nJisell Howe, CDT\n\n\nDec 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstruction Spending in the United States\n\n\n\nposittablecontest\n\n\nrstats\n\n\npython\n\n\n\n\n\n\n\n\nJisell Howe, CDT\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffects of Placed BIM Content Assets on Revit Project Size\n\n\n\nBIM\n\n\nrstats\n\n\npython\n\n\n\n\n\n\n\nJisell Howe, CDT\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "featured.html",
    "href": "featured.html",
    "title": "Features and Publications",
    "section": "",
    "text": "Podcasts\n\n“Construction Dork-ette” panelist on Construction Dorks Podcast for Women in Construction Week\nPanelist on CAD Manager Confessions Podcast: “Women in CAD Masterclass”: Women in Leadership in the AEC Industry\nGuest on CAD Manager Confessions Podcast Episode 8\nGuest on BIM Thoughts Episode 2024\nGuest Podcast Co-host on BIM Thoughts Episode 2026\n\n\n\nWebinars & Virtual Conferences\n\nSpeaker - U.S. CAD Construction Cosmos: “Autodesk Revit Content for the AEC Industry”\nSpeaker - ATG: Midwest University Speaker Series: “Elevate your Power BI Dashboards - Ditch the Defaults in BIM Content Management”\nAU2020 Happy Hour Event Speaker/Panelist\n\n\n\nIn-Person Conferences\n\nFeatured Speaker at IPEX Building It Right Together Conference - Spring 2023\nDunwoody Design + Construction Pivot Conference Speaker: “How Watching Cooking Shows Made me a Better Virtual Communicator in Business”\nAutodesk 2017 Panelist - “Revit Stakeholder Panel - Revit Stakeholders Discuss Standards in Branded Families”\nAutodesk 2017 Co-speaker - “The CAD Manager Wishlist - 2017 Edition”\nAutodesk University 2018 speaker - “Feeling the Pressure: A Case of CAD Manager vs IT (or is it?)”\nAutodesk University 2020 Class Moderator: “CAD Manager Wishlist: 2020 Hindsight Edition”\nMidwest University 2016 speaker\nMidwest University 2017 speaker, 6th highest rated speaker\nMidwest University 2017 Highlight Reel appearance\nMidwest University 2018 speaker\nMidwest University 2018 Highlight Reel appearance\nMidwest University 2019 speaker\n\n\n\nGreater Industry Projects\n\nTechnical Reviewer for the book: “Managing and Visualizing Your BIM Data: Understand the Fundamentals of Computer Science for Data Visualization using Autodesk Dynamo, Revit, and Microsoft Power BI”\nBuilding Product Manufacturer Primary Stakeholder at 2022 AEC Integration Summit\nData Visualization - Tableau Public - Makeover Monday - 52 Consecutive weeks\n\n\n\nContent\n\nAU2020 - AU2022 Sketchnote artist/graphic recorder\nPublished article in July 2017 issue of AUGIWORLD Magazine\nPublished article in July 2018 issue of AUGIWORLD Magazine\nPublished article in October 2018 issue of AUGIWORLD Magazine\nPublished article in July 2019 issue of AUGIWORLD Magazine\nPublished article in November 2020 issue of AUGIWORLD Magazine\nCompany Blog Post - “Visualizing BIM Content Quality and its Impacts”\nCompany Blog Post - “Elevate your Workflow: Computer Mouse Devices”\nCompany Blog Post - “The 2022\nAEC Integration Summit Recap: All Aboard the Train to Connect People and Data”\nGuest Blogger - SolidProfessor: “How to Scale in AutoCAD”\nLinkedIn Newsletter - News You Can Use by ENGworks Global\n\n\n\nGreater Industry Recognition\n\nBuiltWorlds 2021 Adoption Leaders list\nTop 10 Speaker at Midwest University 2017"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jisell Howe, CDT",
    "section": "",
    "text": "I tell 🆂🆃🅾🆁🅸🅴🆂 . . .\nwhere projects are the journey, BIM data is the compass, and the manufacturer client is the hero in continuously exceeding their customers’ expectations when it comes to touch points such as BIM content.\n\nWith extensive first-hand hybrid experience in 🅼🅰🅽🆄🅵🅰🅲🆃🆄🆁🅸🅽🅶 + 🅱🅸🅼, I can confidently tell you the story does not begin with the product catalog, nor does it end with the first download. I work at the intersection to bring out the best of both worlds.\n\nI am a Top 10 Speaker at one of multiple industry conferences I’ve been fortunate to speak at, and my work is featured in AUGIWORLD Magazine, multiple podcasts, numerous webinars, and several other AEC industry avenues.\n\nWhen time allows, I enjoy creating sketchnotes, blog posts, and videos that focus on data visualization (BI), bridging gaps between construction design technology and manufactured products, and other valuable insights in the construction industry."
  },
  {
    "objectID": "posts/a-tableau-breakfast-multicolored-waffle-charts-and-pancakes/index.html",
    "href": "posts/a-tableau-breakfast-multicolored-waffle-charts-and-pancakes/index.html",
    "title": "Breakfast Tableau - Multi-colored Waffle Charts and Pancakes",
    "section": "",
    "text": "I’ve learned a lot from regular data visualization practice every Sunday morning over 16 straight weeks so far. MakeoverMonday even has a submissions tracker that shows where every participant is at every week.\nI’m excited to say that apparently out of nearly a thousand participants, I am one of less than 40 people who seem to consistently participate every single week.\nI can tell that I learned a lot, and I’m happy to show something here that I’m really excited about accomplishing this week.\nHere’s a tutorial that involves the following:\n\nWaffle charts with multiple colors\nA third chart that changes based on two points of interaction from the viewer\nCalculated fields & Parameters\nImage backgrounds, floating elements, and more on dashboards\n\nIn creating my data visualization for this week, some of the items I’ll show are easier than others. Some of them involved a lot of problem solving to tweak it just right. My goal is to show the journey from start to finish.\n\nContext on the Data\nFor this week’s MakeoverMonday topic, I looked at greenhouse gas emissions and what comprises them for various food products. The supply chain activities can vary in emissions for any given product.\nMany might assume that simply “eating local” would impact the amount of greenhouse gas emissions the most. As we’ll see here, transportation is a factor but not the only factor.\nFor this reason, I set out to understand more holistically what is making some products have more greenhouse gas emissions than others. I had a vision in mind for really understanding proportions, hence why I went for a waffle chart. I also had a vision to show relative size. Even though transportation might make up a good portion of a particular product’s gas emissions, the overall emissions could vary greatly compared to another product.\nHere’s how I did it.\n\n\nTutorial – Waffles & Pancakes\nThe first step in accomplishing my vision involved reusing the Waffle chart template I made a while back for another data visualization. There are also downloadable templates online for this as well.\nI connected that template by itself to Tableau Public.\n\nFrom there I poked around a bit to sort how I wanted the waffle chart to look. I placed the Column field on the columns shelf and the Row field on the rows shelf. I then sorted by descending, so then the percentages that will eventually be filled with color would fill up to the top.\n\nAfter that, I went to Data>New Data Source to connect in the actual data source I’ll need to represent within my waffle chart.\n\n\n\n\nCalculated Fields – Ratios\nAfter that, I created calculated fields, very similarly to how I did waffle charts a while back. While the field itself looks a little crazy, the orange part is simply calling what the source and field it is. I happened to create these fields while clicked on the Waffle Chart Tableau data source, so this calculated field is referencing the <source.field> from the other data source. It is taking the emissions that happen during farming and dividing it by the number of records.\nI created these “ratio” calculated fields for all the different supply chain factors.\n***(If you are following this step by step, I just want to note before you get too far that I do end up revising these ratio calculated fields in particular. Scroll down further to Updated Calculated Fields – Ratios header for the actual steps in this part. Keep reading to discover why I show it all anyways.)\n\nHere are a couple tips I wanted to share to make calculated fields go smoother. Many times, the orange print is not something I actually type out. I type a portion of the text and then click on it. Tableau fills in the rest.\n\nAnother tip is to know which fields you’re working with. Notice there are two “number of records” fields. One actually corresponds to our second data set and the other one corresponds to the waffle chart template. Mixing them up can cause issues.\n\nAfter I created my calculated fields for ratios, I created more calculated fields for coloring the “boxes” in the waffle. Here is an example of one of them I created at first.\n\nNotice that this would only account for one color and one supply chain activity. I wanted to show all the colors. So that would be a different calculated field altogether instead. That, I will get to in the next section.\nHere is where the problem lies. It has to do with the number of boxes colored for even just one supply chain activity.\nTo investigate, I right clicked on the data source and selected View Data. What this did was show the raw data that is making up the data visualization. I like to do this to verify that I am indeed showing the data correctly.\n\n\n\nIn this case, I wasn’t showing the data properly. The value 0.80 for Farm does not mean that 80% of emissions came from farming activities. 0.80 is the amount of emissions. If I actually want a proportion, then it needs to be out of the total emissions for that product in particular. In this case, it would be out of 1.4 because it is adding together that row.\n\n\n\nUpdated Calculated Fields – Ratios\nTo remedy the situation, I tried creating a new “Total Column” in a calculated field. This will add up the respective supply chain factors for any given product in the row.\n\nThis Total Column will then go into the ratio calculated fields instead of what was there previously.\n Now the chart shows 57% colored in. Let’s check the math.\n\n\n\nAs I saw from verifying, it looked like it was finally showing the correct proportion. I checked a few other supply chain factors in isolation just to be sure.\n\nNow, to create the color box for all instead of just one supply chain factor, I created a calculated field that looked like this.\n\nWhat this will do is tell Tableau to do different colors for different portions of the waffle chart. I dragged this calculated field to the colors on the marks card.\nFrom there I started to get a little bit into formatting on the waffle chart. Here I show some steps in getting the shapes to line up evenly and come to life.\n\n\n\n\n\n\nCreate Parameters for the Waffle Charts\nNow time for some parameters to help create “selectors” for some interactivity in the data visualization. I right clicked on the blank space in the worksheet and selected Create Parameter.\n\nFor this parameter, I specified that I wanted it to pull from the Food Product list. I ended up making two parameters that were pretty much identical and just called them “Food Product Selector 1” and “Food Product Selector 2.”\n\nAfter that, I created two more calculated fields: “Food Product Show 1” and “Food Product Show 2.” Notice the orange and purple text. The orange is just a regular field in the data and the purple calls out the parameter name. \nThe “Food Product Show 1” calculated field was then placed on the filters card on the first waffle chart. I duplicated the waffle chart worksheet and then placed “Food Product Show 2” in the filters card instead.\n\nSo now we have food product selectors that will eventually control two waffle charts. Wouldn’t it be neat if we made those selectors each control a different chart altogether as well? Well, don’t mind if I do!\nHere’s a bubble chart that shows total emissions for each food product. Or we can call them pancakes, just for fun. The screenshot currently shows Food Product on the filter shelf with two foods selected. But we want the interactive selectors to make those selections for us instead on the fly!\n\nHere is a newly created calculated field that says that if the parameter selector comes up with a valid food product, then show that product. It will also do the same for the second product simultaneously. I placed this calculated field on the filters card instead of the Food Product field that was originally there.\n\nAt that point, I just specified that it needs to be true in order for it to work the way I wanted. That means that the parameter selectors I created must each pick out something that exists and display here.\n\n\n\nDashboard Design for Waffles & Pancakes\nThis data visualization really involved a lot of different things to make it work. Each individual feature would not be able to function without the others. Here is how I put it all together in the end.\nThe snippet below shows that I started putting the two waffle charts onto the dashboard. I specified for each Food Product Selector parameter to show up in the form of a floating compact list. What this will do is create a drop down menu and let me place it wherever I want.\n\n\n\n\nActions for the Waffle Charts\nInteractivity in a data visualization is cool. It’s even cooler if you can get one chart to then affect the view of another chart at the same time. That’s what I used actions for here.\n\nIn this case, I created two parameter actions. I wanted each parameter selector to be able to change the display of the waffle chart. We know this will work because we already have a calculated field filtering each waffle chart for the results.\nIn one parameter action, I check the box for one waffle chart as a source. The other parameter action specifies the other waffle chart.\n\nThe bubble chart functionality is already taken care of because of the other calculated field filtering it on its respective worksheet. That means that the viewer will select from the drop downs to make the waffle charts change. And then however the waffle charts change will affect the bubble chart. It’s sort of like a chain reaction. Pretty neat.\nNow onto finessing the data story we want to tell.\n\n\nData Storytelling in Tool tips\nAs I’ve mentioned in at least a few of my tutorials, Tableau tool tip notes basically start out as a box full of generated data fields. It doesn’t help the viewer very much because it’s not written in familiar language. So I often change my tool tips to be more readable.\nHere is the tool tip I customized on the bubble chart worksheet. This will help us communicate why one bubble is larger than another when they are being compared.\n\nHere is the tool tip I customized on each waffle chart. What this will show is the broken down percentages of each supply chain activity. Producing a certain food product could have a certain breakdown of factors but will result in a total amount of emissions.\n\nOne issue I initially noticed when hovering over my waffle chart was that it was simply accumulating percentage as I moved the mouse over every circle in the waffle. We know that’s not correct, so I removed that and just included a list instead of all the “ratio” calculated fields I made earlier. This will give me percentages in the tool tip.\n\nI took it a step further and formatted the fields to read as actual percentages instead of just decimal numbers.\n\n\nThe Final Data Visualization\nAfter all of that work, here is what I ended up putting together. I added a floating background image on the dashboard from Pixabay to complete the vision I was going for. I did this via the drag and drop options in the bottom left of the dashboard view.\nNow the viewer can compare a food product in list one and a food product in list two. Those two selections will display two different gas emission waffle charts. And then the bubble chart will display the total emissions for each two products selected. Try it for yourself here in the interactive data visualization! It works best viewed on a desktop or tablet.\nIn the screenshot below, it is comparing “Beef (beef herd)” and “Berries & Grapes.” Even though transportation seems to be more of a factor in the production of berries & grapes, beef still has way more greenhouse emissions. We can see from the legend that this is due to the farm aspects of the supply chain. Here’s more context on the why."
  },
  {
    "objectID": "posts/and-action-how-to-invite-the-viewer-to-explore/index.html",
    "href": "posts/and-action-how-to-invite-the-viewer-to-explore/index.html",
    "title": "And Action! How to Invite the Viewer to Explore",
    "section": "",
    "text": "One of the cool things about data visualizations is that they can be interactive to the viewer. Anything from tool tip text to specific things that happen when you hover or click on certain aspects of a visualization can create a truly immersive experience for the viewer. This puts the viewer in control of the action instead of just passively looking at a picture.\nAs I listened to this episode of Data Viz Today on how to fill your data viz toolbox, I realized that there are different goals you can have for creating data visualizations. One of those goals is if you want your viewer to be able to ask and answer their own questions through exploring your data visualization. If so, we would need to be able to build in functionality to an interactive data visualization to allow just that.\nHow can this be done? Within Tableau, there is a feature called Actions.\n\nWhat is an Action?\nAn Action basically tells a data visualization to do something specific based on what the viewer of the visualization does while viewing it. There are five different kinds of Actions that allow for a lot of customization.\n\nFilter Actions – Whatever the viewer clicks on/hovers over in one visualization, it will then filter within another visualization\nHighlight Action – Whatever the viewer clicks on/hovers over in one visualization, it will then highlight the selection within another visualization\nURL Action – This triggers a link to a website, YouTube video, picture, or email from the dashboard\nGo to Sheet Action – This allows the viewer to jump from one dashboard to another.\nSet Action – This basically allows the viewer to change the values in a set so a visualization is updated on the fly, such as coloring things that weren’t colored before.\n\nLittle overview demos on each one are showcased on this great YouTube Video here. My focus in this post today is on the Highlight Action as well as the Filter Action.\nKeep reading to learn how to harness the power of Actions!\n\n\nThe Context on the Data Set\nMakeoverMonday Week 4 did not disappoint in providing background for its data set. This week’s data set covers the impact of an organization called Bridges to Prosperity. What they do is build bridges around the world in isolated communities to give people safe access to education, markets, employment, and health services. Just a single footbridge across an impassable river can impact an entire community.\nThe goal for my data visualization was to show the impact of a single bridge on average by analyzing the number of people served.\n\n\nInitial Data Exploration\nTo start off, I selected a map to plot my data points, but as you can see, the points seem to get lost on the map. My efforts in changing the size and color based on the number of bridges built seem to not matter much at first.\n\n\nAt this point, I decided to make the background map take on a dark theme. This seemed to help the different areas pop out much more.\n\nI then shifted my focus to the tool tips. Recall from my previous posts that the default tool tips seem to show just a bunch of variables and numbers. I like to change these to read like actual sentences.\n\n\n\nFiguring out the Impact of a Single Bridge\nLooking at overall numbers is great, but I thought it might be neat to zero in on the impact on a much more granular level. I right clicked in the white space under Measures and selected Create Calculated Field.\n\nWithin this new Calculated Field, I created a calculation called “Impact Ratio” to just take the sum of the number of people directly served and divided by the number of records. What that does is tell me how many people on average have been served per bridge in any given situation. The original data set was structured in such a way that it showed each bridge on its own single line. Therefore, this calculation could work when it is placed within the data visualization.\n\nThis is actually one of the nice parts I like about Tableau. I love that I can create different calculations while I am working within the worksheet, so I can drag and drop it right away to see its effects on the visualization. I don’t need to create anything special in the data set beforehand.\nThis time I formatted a different tool tip to read a different insight: the impact of a single bridge.\n\nNow for the fun part. Tableau Actions!\n\n\nHow to Create an Action\n\nAs I mentioned earlier, there are five different kind of actions. The kind that I will pick out this time is Highlight Action.\n\nI then specified the following to happen:\n\nIf the viewer hovers over something in the Distribution visualization (Source), then it will also highlight in the Impact visualization (Target)\nIf the viewer hovers over something in the Impact visualization (Source), then it will also highlight in the Distribution visualization (Target)\n\n\nI specified All Fields and didn’t really spell out anything else for this particular Action. As you can see, there are a lot of different options even within the Action types!\nI then decided I wanted to not only highlight what was hovered over. I wanted to filter by that as well.\nSimilar to the Highlight Action, I specified that I wanted to have something happen with the Distribution visualization when I hovered over it in the Impact visualization.\n\n\n\nThe Final Results\nHere is the result of these two actions. When I hover over a country in the bottom visualization, the top visualization filters actually moves around to that part of the world to show it more clearly.\nAdditionally, if I hover over the map somewhere, it will simply highlight the country in the bottom visualization. I didn’t use the Filter Action go the opposite way on this one because it would have zoomed way in on just one bar in the bar chart, which wouldn’t really serve much of a purpose.\n\nWhen nothing is hovered over, this is what the final visualization looks like. \nHere is a link to the actual interactive visualization that show the Actions …in action!\n\n\nLessons Learned\nIf I were to revisit this data visualization again, I think I might look into the actual bridges themselves to provide more specific insights on the impact of a single bridge. Sure, there could be X number of bridges in one country, but they aren’t all created the same. Some are longer or shorter. Some could individually serve more than others.\nAlso, I’d like to look into more on which bridges were completed versus under construction and confirmed. My data visualizations here only looked at the completed bridges. Maybe there could be some insights on where the organization could build next.\nFinally, I think I would revisit the colors. The decision was largely based on the “dark theme” I specified for the map background. Maybe I could have gone with more green in the gradient to go along with the organization’s color scheme.\nI believe that constantly reiterating and reflecting on our work will help us produce better work as we go forward. I encourage you to do that in all your endeavors as well!"
  },
  {
    "objectID": "posts/bringing-it-into-perspective-with-shapes/index.html",
    "href": "posts/bringing-it-into-perspective-with-shapes/index.html",
    "title": "Bringing it into Perspective with Shapes",
    "section": "",
    "text": "Have you ever seen percentages for a population or for those who took a survey and wondered just how many people the percentage was talking about? Or maybe the number seems so large, that it’s difficult to put it in perspective?\nShapes in Tableau can help. Here’s a tutorial that shows how to basically say, “if there were actually only 100 people in this data set, then this is how many would do X as opposed to Y.”\n\nWaffle Chart Set up\nHere’s the basis of how to get started. This here shows that I connected a waffle chart template as referenced in previous posts along with the data set. From the waffle chart template, I placed the Row field on the Rows shelf and the Column field on the Columns shelf.\nFor this particular data set, it involved coming up with proportions for daily, weekly, monthly and more consumption of animal-free products. For the sake of this tutorial, you’ll see just one of these charts and what had to take place to make it work.\nSo I right clicked, created a calculated field, and then typed in this syntax. It basically says that if my data value exceeds a percentage value from the waffle chart template, then show it.\n\n\n\nShapes\nThis boolean T/F calculated field was then placed on the Shapes of the Marks Card. From there we can click on the Shapes to change the shapes to be something else. The “True” shape will be a person shape that shows up out of box with Tableau. We won’t worry about the “False” shape quite yet.\n\n\n\nI adjusted the size a bit and scooted the distributions of the people shapes to be closer together. Because we don’t want the “False” shapes showing at all, I then placed that same calculated field I just created onto the Filters shelf to filter for only “True.”\n\nJust for now I placed the Category field also on the Filters shelf to filter by the consumer type. It will then only show the percentage of Vegan people who consume animal-free products daily. Without this filtering, the percentages can get goofy. Later on, I’ll show how to let the viewer pick out what they want to filter with just a click.\n\n\n\nTool Tips\nI briefly moved on to tool tips to format a bit better. Be careful here because if you don’t put the right field in the tool tip, it’ll accumulate percentage numbers in the dynamic tool tip. We want it to show the correct percentage no matter where you hover. Shown below is simply the “percentage,” which isn’t the right field to use.\n\n\nHere’s the actual percentage field we need. In this case it was the “Daily” field that was formatted to read like a percentage.\n\n\n\n\nParameters & Actions\nAs mentioned previously, there is a way to let the viewer pick out how they want to filter and view things automatically. We could have certainly left a filter on the dashboard, but I happen to like parameters & parameter actions for use cases like this.\nI right clicked on the left side and selected Create Parameter. From there I populated the prompt as shown so it would give the viewer options to pick from.\n\nOn the dashboard itself, I went to Dashboard > Actions to create a parameter action. What this action will do is affect what gets filtered based upon user user selection.\n\nTo put it altogether, we need a calculated field on the worksheet to do the filtering for us. So now, instead of the Category field on the filter, we will put this particular calculated field here instead. This calculated field basically says to show whatever is selected from the parameter we created.\n\nWe set the “Consumer Filter” to be “True” so it will filter and show only exactly what the viewer selects in the parameter and nothing else. Here’s what I mean.\n\n\nThis same calculated field can be used on other worksheets as well.\n\n\nAnnotations\nIn this case, I used it to show an exact percentage as an annotation that will go on the dashboard. Because the Consumer Filter calculated field was also placed on the Filters shelf, the percentage shown below will also change with the parameters and parameter actions for the viewer. Here I have the “Daily” field formatted as a percentage and placed on the Text of the Marks card on a new worksheet.\n\nHere’s the start of the layout for these percentages next to the corresponding people shapes.\n\n\n\nThe Final Data Visualization\nHere’s how the final data visualization turned out. Because the exported image looks a little derpy, here is the link to the interactive data visualization and below this is a gif showing how it works."
  },
  {
    "objectID": "posts/dont-stop-me-now-animating-data-insights/index.html",
    "href": "posts/dont-stop-me-now-animating-data-insights/index.html",
    "title": "Don’t Stop Me Now - Animating Data Insights",
    "section": "",
    "text": "About a month ago, Tableau fully introduced data #VizAnimations outside of beta in its 2020.1 release. I thought it would be really fun to try animating a data set that had changing values over time.\nThis week’s MakeoverMonday challenge looked at educational offerings by Berkeley College over the last 100 years or so. What I found really interesting was that the educational courses that were taken seemed to correlate with big events and eras in history.\nHow would one know about these trends over time?\nBy animating the changes over time in the Berkeley data set, I am able to see exactly how everything shifted around and grew as the years pass. Often times, if we are looking at multiple static graphs, we need to study the differences. Not in this case!\n\nContext Behind the Tutorial\nPrior to coming to the conclusion of animating this data set, I was struggling a bit while brainstorming. I was trying to figure out another way to show proportions that was not a pie chart. Why not a pie chart?\nWell, pie charts themselves might not be too bad if there are only a few items in a proportion, but there are a lot of pieces in the original charts that make it a challenge to gather useful insights.\n\n\n\nImage from interactive dashboard: UC ClioMetric History Project\n\n\nLearn how to animate your insights with Viz Animations.\n\n\nTutorial\nOnce I opened up Tableau Public and connected the data source, I created a bit of an exploratory workbook to figure out just what is going on in this data set. Often times, you’ll find that you can’t just poke around and start visualizing things right away. You need to prep data when it’s messy and explore what is going on with it.\n\nIn this case, I wanted to find out how exactly these classes were organized in a hierarchy. It looked like there were names of classes, fields, an educational area, and a more broader general area. There were also other factors such as when the classes were taught, whether it was just listed or actually taken, and more.\nThis exploration helped me see that I wanted to break these down by general areas to try to cut down on the visual complexity. Here are some default colors that popped up.\n\nI happened to make mine fit more of a colorblind friendly color scheme, but either way I personally think getting down to six colors is already a win.\n\nNow for the fun part. After I initially set up my dashboard with a fixed size, I went to Format>Animations to turn on animations!\n\nThen, I went over to the workbook itself to drag Year to the Pages card and to the Filters card. What this will do is let me pick out which years I want in this animation. It also allows me to pick out how I want the animation to behave.\n\nI ended up selecting every five years, so I could get a good idea of what happened during each decade.\n\nWhatever I selected in the filters card ended up in this little drop down list. This little card lets the animation work.\n\n\n\nFinal Touches\nTo add a final touch, I modified the tool tips in the workbook to read more like sentences. Here’s how that turned out.\n\nHere is the final data visualization with added text on the side in the dashboard. I dragged and dropped “text” and “blank spaces” in a tiled way on the dashboard to rearrange everything to be like what I wanted. Now we’re ready to animate in the final visualization!\n\nInteractive dashboard available here\nHere’s the animated data visualization! Enjoy!"
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "",
    "text": "It has been over a month since I purchased a license of iA Writer. Regrettably, I have not spent time exploring this fantastic tool as much as I should have as I was busy with personal projects: one of which involving a migration of my website from WordPress to Quarto/GitHub Pages. Before I get into a review of iA Writer, I want to provide more context that led to the decision in purchasing a license in the first place."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#background",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#background",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Background",
    "text": "Background\nMany of us are familiar with Microsoft Word and the suite of products that comes pre-installed on Windows operating systems. Those who are digital natives might not know of anything different.\nWhat you might not know is that Microsoft Word and similar products are known as “What you see is what you get” or WYSIWYG editors with buttons for visual picks and clicks (WYSIWYG, 2022).\nWhile these editors are great in their own ways, they also come with challenges for some."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#challenges",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#challenges",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Challenges",
    "text": "Challenges\nTables that are difficult to integrate…\nText that randomly changes fonts in the middle of writing…\nInconsistent indentations with an unknown cause…\nThese are the things that have been sticking out a lot more to me recently.\nFamiliarity with a product or process, despite its challenges, is akin to the another familiar phrase that is painted as the “villain” in many business books on innovation: “We’ve always done it this way.”\nBusinesses as a whole are not the only places where we can question the status quo. We can do that with our own personal workflows."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#evaluating-workflows",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#evaluating-workflows",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Evaluating Workflows",
    "text": "Evaluating Workflows\nFor about a year and a half, I have been evaluating how much time has been going into various activities in my day-to-day life with the help of Clockify. I do it to build business cases for process improvement as well as for personal insights.\nI found that a decent amount of my time falls under administration and management activities. A wise person once told me about investing in optimizing the things I use the most. The application of that advice was intended for things such as investing in comfortable shoes or a nice bed mattress. But I think it applies here too.\nProductivity gains in these areas are essential to make room for more value add activities. The logic is that we can focus more on the “why” of what we do once we optimize the “how.”\nFor this reason, I looked for ways to improve upon my own personal workflows, up to and including rethinking the concept of word processing altogether.\nWYSIWYG editors may be presented as the only option. However, they are simply a default or common option. When you think about it like that, it opens up a new realm of possibilities for those who may want to try something different."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#introduction-to-markdown",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#introduction-to-markdown",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Introduction to Markdown",
    "text": "Introduction to Markdown\nMy discovery of Markdown came in perfect timing.\nI was in the middle of watching a replay of an R-Ladies Paris Meetup for an intro to {ggplot2} with {R}. The workflow for the meetup involved exploring RMarkdown (.Rmd) files and running snippets of {R} code.\n\n\n\nR-Ladies Paris: Intro to {ggplot2}\n\n\nI was fascinated by the nicely formatted RMarkdown documents. Who knew that a couple ## could denote headers, hyperlinks could be conveyed with brackets and parentheses, and text could be emphasized with * to denote bold weights and italic styles without my hands ever leaving the keyboard. And there are plenty more examples.\nFrom there, I wondered if it was possible to incorporate Markdown in “everyday situations” beyond coding and professional scientific reports.\nAfter going down further rabbit holes, I learned that multiple “flavors” of Markdown exist (Faraday Academy, 2021). I then quickly realized it could get complicated if you didn’t have a means to properly render those documents for a desired output.\nFor instance, I casually and briefly tested Obsidian, which could be excellent for personal notes, but it didn’t appear to be an ideal solution for creating everyday documents that needed to be readily exported to .docx. A similar situation was the result when I tried writing Markdown in VSCode. It’s not that it couldn’t be done. The methods and goals were different from what I wanted."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#refining-my-goals",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#refining-my-goals",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Refining My Goals",
    "text": "Refining My Goals\nMy conditions for a viable alternative to a WYSIWYG editor for “everyday situations” involved capabilities such as exporting to .docx, .pdf, and .html in a way that I found acceptable and without jumping through a lot of hoops to do so. The viable solution also had to have “previews” of my formatted text and support for a variety of Markdown features.\nOne of the most helpful resources in making sense of the possibilities was the Markdown Guide. Not only did I purchase Matt Cone’s book on the topic, I learned that not all Markdown editors are created the same (Cone, 2022). The tools section on the Markdown Guide website was incredibly valuable in helping me determine my next steps.\nNow that you have a lot more background on how I got here, I will now go into a review of what I like about iA Writer, my Markdown editor of choice (aside from using RStudio and Quarto, of course)."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#syntax-highlighting-and-style-check",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#syntax-highlighting-and-style-check",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Syntax Highlighting and Style Check",
    "text": "Syntax Highlighting and Style Check\nMy website is currently hosted on GitHub Pages. If you look at my “contributions,” you’ll see that I’m incredibly “active” in updating my blog posts. A lot of this has to do with me finding things I want to reword after publishing. I create my blog posts with RStudio/Quarto. What I have noticed so far is that beyond basic spell check, the RStudio IDE is not necessarily geared for refining prose.\nIf I were to compare iA Writer and RStudio, I’d say that while both can support code and prose, iA Writer focuses more on prose, and RStudio focuses more on code. There is nothing wrong with that. It’s all about being aware and using tools for their perceived strengths.\niA Writer provides a great Syntax Highlighting tool for highlighting adverbs, verbs, conjunctions, nouns, and adjectives. In addition, it can check for clichés, fillers, redundancies, and even a special custom list of words that you don’t want to use (iA Writer, 2022).\n\n\n\nSyntax Highlighting and Style Check from iA Writer"
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#clean-and-simple-interface",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#clean-and-simple-interface",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Clean and Simple Interface",
    "text": "Clean and Simple Interface\nWith a click of a button, you can switch to “Night Mode,” which will make the entire user interface turn a darker color and leave all the colored text. This is pleasing to the eye as someone who has taken a liking to a “coding” environment in various color schemes.\nI like the “Full screen mode” where the top “ribbon” stays out of view until you move your mouse up to the top of the screen. This is a huge contrast compared to a large ribbon that shows up by default in Microsoft Word, for example.\nIf you forget any Markdown syntax, you can use the Format drop down menu to pick and click various formatting options or gain insights on keyboard shortcuts.\nFrom my current understanding, there are three main sections of the user interface:\n\nLibrary\nMain typing area\nPreview Pane\n\nVarious sections can be turned off. I personally like leaving the Preview Pane turned on because I can see what my Markdown formatted text looks like. In addition, I like having the Library view turned on. I can drag and drop images and see all the headers in my writing thus far. I can click and jump to different headers that I created with Markdown syntax (example: #, ##). And speaking of headers, a small but nice touch is that the # part of the header syntax places itself outside your writing, as shown with the “Snippets” header. That’s usually not what I see when writing Markdown syntax in other editors.\n\n\n\nNight Mode user interface; drag and drop images in iA Writer"
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#snippets",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#snippets",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Snippets",
    "text": "Snippets\nSnippets are a great way to save time in placing text.\nMy current understanding is that you come up with a “short-code” that represents the full text you want placed. Hitting the Tab key after you type your short-code will result in the full text taking its place.\nAmong the default ones included in iA Writer, here is an example of one I created for a sign off cadence.\n\n\n\nSnippets in iA Writer"
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#tables-and-variables",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#tables-and-variables",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Tables and Variables",
    "text": "Tables and Variables\nThe focus of this review has been centered more around specific features that I think set iA Writer apart instead of on Markdown itself. I would be remiss if I did not include what I think is an amazing feature that takes it further than what Markdown offers on its own.\nTables can be created in Markdown with a series of characters like these |—|.\n\n\n\nColumn1\nColumn2\nColumn3\n\n\n\n\nValue1\nValue2\nValue3\n\n\n\niA Writer will generate a table with a left justification with the syntax above.\n\n\n\nSide-by-side comparison of writing pane and preview pane in iA Writer\n\n\nThe part that I think is an absolute game changer is integrating the Markdown table syntax with the variables that iA Writer supports. To be clear, this is an added feature in the iA Writer platform beyond typical Markdown.\nAccording to the iA Writer Markdown User Guide, there is a section on Metadata that talks about support for variables.\nAt the beginning of your document, you can set variables such as a customer name, date, and more. Then, every time you use those variables throughout a document, iA Writer will put in the correct value. To be fair, document properties do exist in Microsoft Word to allow for this, but the options appear to be limited. In iA Writer, you create any variable you want within seconds (iA Writer, 2022).\n\n\n\nVariables in metadata in iA Writer\n\n\nIf we were to revise our table and use the correct syntax for variables, we’d now have the following:\n\n\n\nColumn1\nColumn2\nColumn3\n\n\n\n\n4\n2\n6\n\n\n\nNote that this feature appears to be turned off by default in iA Writer. You can turn it on to recognize processing of metadata in Preferences. Also note that this is one of those features that will not automatically transfer to other Markdown editors.\n\n\n\nVariables within tables in iA Writer"
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#output",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#output",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Output",
    "text": "Output\nBy now, I hope I have opened your eyes to the possibilities using Markdown for your everyday writing needs. The most important part about any viable solution is the output.\niA Writer offers export to HTML, PDF, Markdown, and Word. In my experience, the exports have looked exactly as I would have expected without surprises of things formatting differently upon export."
  },
  {
    "objectID": "posts/exploring-everyday-possibilities-in-markdown/index.html#now-what",
    "href": "posts/exploring-everyday-possibilities-in-markdown/index.html#now-what",
    "title": "Exploring Everyday Possibilities in Markdown",
    "section": "Now What?",
    "text": "Now What?\nAt the time of this writing, iA Writer costs $30 for a license. In my opinion, it is well worth it. However, iA Writer is not the only tool out there. In fact, some of the tools you may use today may even support Markdown to varying degrees such as MS Teams and Google Docs. There are also online tools such as Dillinger if you wanted to try it out for yourself before finding the best tool for you. The key is to understand what your goals are, and then you can build the solution around that. Maybe Markdown is a solution for you.\n---\nSources Cited:\nAcademy, F. (2021, March 24). Learn markdown in 30 minutes! YouTube. Retrieved December 4, 2022, from https://www.youtube.com/watch?v=bTVIMt3XllM\nCone, M. (n.d.). Markdown Guide. Retrieved December 4, 2022, from https://www.markdownguide.org/\niA Writer. (2022, November 28). Retrieved December 4, 2022, from https://ia.net/\nParis, R.-L. (2022, October 14). Introduction to {ggplot2} in R by Tanya Shapiro | R-ladies paris. YouTube. Retrieved December 4, 2022, from https://www.youtube.com/watch?v=EnNWkF9Jtj4\nWikimedia Foundation. (2022, November 10). Digital native. Wikipedia. Retrieved December 4, 2022, from https://en.wikipedia.org/wiki/Digital_native\nWikimedia Foundation. (2022, November 23). WYSIWYG. Wikipedia. Retrieved December 4, 2022, from https://en.wikipedia.org/wiki/WYSIWYG"
  },
  {
    "objectID": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html",
    "href": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html",
    "title": "How to Verify your Quarto Blog on Mastodon",
    "section": "",
    "text": "I first signed up for a Mastodon social account a few weeks ago to more closely follow the data science community. So far, I am really enjoying the highly curated (ad free) feeds that focus on the topics I care to learn about. I noticed there was a feature within Mastodon that could verify links such as your website, so others could see that the same person is controlling both an individual Mastodon account and a particular linked blog website. It results in a fancy green check mark next to a website you list in your profile, and it doesn’t cost any money to do so."
  },
  {
    "objectID": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#verification",
    "href": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#verification",
    "title": "How to Verify your Quarto Blog on Mastodon",
    "section": "Verification",
    "text": "Verification\nAt the time of this writing, the Mastodon documentation has this to say about verification:\n\nDocument-based verification and blue ticks are not possible without a central authority. However, Mastodon can cross-reference the links you put on your profile to prove that you are the real owner of those links.\n\nVerifying your website with Mastodon involves adding what I understand to be a microformat attribute called rel=\"me\" to the HTML in the back end, so Mastodon can cross-reference and verify that the two are connected."
  },
  {
    "objectID": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#the-old-challenge",
    "href": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#the-old-challenge",
    "title": "How to Verify your Quarto Blog on Mastodon",
    "section": "The Old Challenge",
    "text": "The Old Challenge\nWhile some tutorials I found on this topic talked about doing this for a WordPress site, I didn’t find them immediately applicable as I recently transitioned away from having a WordPress site.\nA big reason why I transitioned from having a WordPress site was that it involved so many (overwhelming) bells and whistles, and I think it actually hindered my learning process of building/maintaining a website. The powerful tool quickly surpassed the pace that I was going in learning the ins and outs of it. If I still had my WordPress site, I probably would have tried copying/pasting the example HTML provided verbatim somewhere, which at best probably would have had some random link with text show up separately from all my other social links. The look could be cleaner, in my opinion."
  },
  {
    "objectID": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#the-new-challenge",
    "href": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#the-new-challenge",
    "title": "How to Verify your Quarto Blog on Mastodon",
    "section": "The New Challenge",
    "text": "The New Challenge\nTransitioning to a Quarto blog/site allowed me to build the most critical aspects of a website and closely study what each file did along the way. Adding some HTML for Mastodon verification seemed like the perfect challenge for me to understand applications of HTML a bit more beyond the basic syntax I understood already.\nGiven that my main “interface” for creating website pages is now not a complex online “editor” but the rather lean and mean RStudio, I will fill you in on my thought process and path to verifying a Quarto website with Mastodon in a very clean way."
  },
  {
    "objectID": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#thought-process-and-method",
    "href": "posts/how-to-verify-your-quarto-blog-on-mastodon/index.html#thought-process-and-method",
    "title": "How to Verify your Quarto Blog on Mastodon",
    "section": "Thought Process and Method",
    "text": "Thought Process and Method\nMy now current workflow for creating blog posts and pages for my Quarto blog/site involves creating .qmd files, telling Quarto to render everything, copying the entire “docs” output folder to my GitHub Desktop location, and deploying the “docs” folder to GitHub Pages. By the way, this workflow is explained further with linked documentation in my previous blog post in case you are interested in doing the same.\nWhen you set up a Quarto site, you will by default generate several files including an about.qmd file that can later be renamed to an index.qmd file. This is just one of the files that offers a place to add navigation links to other pages and social links. The file below will render to index.html.\n\n\n\nImage of the frontmatter of the index.qmd file from Quarto documentation\n\n\nTo incorporate what is needed to verify my website with my Mastodon account, the additional rel=\"me\" syntax ultimately needed to end up in the rendered index.html file of my website.\nTo do so, I did the following:\n\n1. Go to Edit Profile in Mastodon\nI went to the Edit Profile area of my Mastodon account where I could add up to four links. Next to those four links is a special HTML syntax with instructions to add rel=\"me\" next to the Mastodon profile link within the HTML of the website I wanted to verify.\n\n\n\nImage from Mastodon>Edit Profile\n\n\nAn example of this is below:\n<a rel=\"me\" href=\"https://MASTODONINSTANCE/@USERNAME\">Mastodon</a>\n\n\n\n\n\n\nNote\n\n\n\nThe critical parts are the rel=\"me\" and the link to your Mastodon profile page. The display text Mastodon isn’t necessary. Also, the rel=\"me\" attribute can also show up after the link as opposed to before it as an option.\n\n\n\n\n2. Add the rel=\"me\" Attribute\nI located my rendered index.html file to add the rel=\"me\" part next to the Mastodon social link within the HTML. In my posting workflow, that file is located in my “docs” folder. What this does is let Quarto continue to render the buttons and text exactly as intended, and there is no extra text talking about “verification” anywhere.\n\n\n\nImage of link buttons that can render with Quarto\n\n\nWhile that does indeed work for verification purposes in further steps below with Mastodon, perhaps a better route to further explore is adding the rel=\"me\" attribute next to the Mastodon link and other social links in the index.qmd file itself with customized HTML. This will ensure it sticks every time index.html is rendered going forward, and it offers an opportunity to learn how to style it nicely.\nHere is one basic example of creating a similar button link that takes on the same styling as the rest of the Quarto website:\n<form action=\"https://MASTODONINSTANCE/@USERNAME\" rel=\"me\">\n  <button type=\"submit\">Mastodon</button>\n</form>\nAnd here is the functioning result (not a screenshot):\n\n  Mastodon\n\n\nOnce the link and rel=\"me\" attribute were added to the index.qmd file, I rendered it.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s important to verify that the rel=\"me\" attribute has been added to the index.html file once rendered. You can do this by right clicking and inspecting the page.\n\n\n\n\n3. Deploy onto GitHub Pages\nOnce I rendered the files, I deployed them all onto GitHub Pages. After a few minutes, I was able to inspect the contents of my website on the front end to see that the rel=\"me\" was reflected next to the Mastodon social link. With that done, Mastodon recognized the website link as “verified,” and a green check mark appeared next to it in my profile.\n\n\n\nBonus Verification - GitHub Account\nYour personal website isn’t the only site that can be verified with Mastodon. I recently learned of a way to verify my actual GitHub account as well. This post that I happened to come across on Mastodon gets into details on setting up a repository that has one HTML file that redirects to your GitHub profile page. Within that HTML file, you add the rel=\"me\" syntax as well. This makes it so Mastodon will verify your GitHub account with another green check mark.\nI followed the workflow myself from the post and saw it working very well. My only edit is that I only included enough HTML syntax for the noted essential items in that post because I want to be more intentional with my learning experience. Copying/pasting entire snippets without understanding fully what they do can make it more of a challenge to apply learning and connect the dots.\nGoing through this exercise in verifying my website and GitHub account was a great learning experience. I am looking forward to more interesting challenges.\n\nSources Cited\nHow to add a rel attribute to all links in WordPress. SupportHost. (2022, November 10). Retrieved November 19, 2022, from https://supporthost.com/rel-attribute/\nLet’s build a blog with quarto. quarto. (n.d.). Retrieved November 19, 2022, from https://ivelasq.quarto.pub/building-a-blog-with-quarto/lets-build-a-quarto-blog/#about.qmd-blog-about-page\nQuarto - Markdown Basics. Caption. (n.d.). Retrieved November 20, 2022, from https://quarto.org/docs/authoring/markdown-basics.html\nREL=“Me”. rel=“me” - Microformats Wiki. (n.d.). Retrieved November 19, 2022, from https://microformats.org/wiki/rel-me\nSetting up your profile. Mastodon. (n.d.). Retrieved November 19, 2022, from https://docs.joinmastodon.org/user/profile\nWilkins, J. (2021, April 28). HTML button link code examples – how to make HTML hyperlinks using the HREF attribute on tags. freeCodeCamp.org. Retrieved November 20, 2022, from https://www.freecodecamp.org/news/html-button-link-code-examples-how-to-make-html-hyperlinks-using-the-href-attribute-on-tags/\nWillison, S. (n.d.). Verifying your github profile on Mastodon. Simon Willison’s TILs. Retrieved November 19, 2022, from https://til.simonwillison.net/mastodon/verifying-github-on-mastodon"
  },
  {
    "objectID": "posts/its-the-little-things-small-multiples/index.html",
    "href": "posts/its-the-little-things-small-multiples/index.html",
    "title": "It’s the Little Things - Small Multiples",
    "section": "",
    "text": "The Data community is great for seeing all sorts of artistic and informative displays of information. One such display I see fairly often is in the form of small multiples or tile maps.\nHere I will show you how I created one that shows small multiples of pie charts in the layout of the U.S. states.\nI know I have touched base on pie charts before and how they aren’t really ideal for data visualization. However, I really don’t want to rule them out completely. There are many opinions on this, and I think at this time I am in the camp of “it depends.”\nIn the tutorial below, I’ll show you that I used pie charts to highlight just ONE aspect of a proportion. It seemed to really bring out some insights that some people didn’t even notice initially. But first, a little context.\n\nContext on the Data\nFor this week’s #MakeoverMonday practice, I looked at car insurance rates across the U.S. There is a minimum car insurance rate and a full/comprehensive coverage amount. Many people who participated in this exercise were able to see that Michigan has really high costs because of it’s “unique requirement of no-limit personal injury protection.”\nBut what is the deal with Florida? The pie chart tile map I created for this exercise showed me that the state’s minimum car insurance cost makes up a whopping 76.11% of its full coverage cost. Quick screenshot below for reference. Here’s the reasoning why if you are interested.\n\nAnd now for a tutorial.\n\n\nTutorial\nThe layout for the tile map was based on a template that Brittany Fongcreated, so right away I’d say to get a hold of that template. I had a bit of a hard time downloading the exact template file, so I ended up locating one of the data vizzes on Brittany Fong’s Tableau public profile and downloading the “data source” which was the template.\n\nAfter that, I connected both that template and my actual data file to Tableau Public.\n\nAs mentioned in multiple other kinds of small multiples tutorials, we’ll need to edit blend relationships. What this will do is tell Tableau how to relate these two data sources. In this case, we want it to understand that State in one data source will be referring to Abbreviation in the other data source.\n\n\nFrom here you can see that I placed the Row field from that template data source on the rows shelf and the Space field on the columns shelf. I also made the marks card display a Pie with Measure Values (difference & minimum coverage) as the “angle” slices.\n\nAt first the layout did not work for me at all and I was so confused as to why. Then I saw that the “template” I downloaded had “abbreviations” of states but not the state names. My data source had only state names. So in this case, I right clicked on that data field and created aliases. In a different situation, maybe I would have done something differently, but this is just what I did here because I was desperate while troubleshooting to keep things moving.\n\nNote that the aliases need to match EXACTLY for this to work. At that point, Tableau understood what was going on from the blending relationships because I made sure the aliases matched the field in the other data source.\n\nFrom there I explored many formatting options because they are endless in Tableau. One of my challenges in formatting the tool tips was that I wanted it to show percentages of minimum insurance rates per pie chart. Usually, what I see it doing by default though is the percentage of all the data in the workbook. That’s not what I wanted here.\n\nWhat I ended up doing was creating a calculated field and formatting it as a percentage once placed on the details of the Marks card instead. This would allow me to put the calculated field in the tool tip for a percentage instead of the % of Total Sum (minimum coverage) that was there previously.\n\n\n\nThe Final Data Viz\nHere’s what I came up with for my “final” data visualization. And here’s the interactive data visualization.I was able to find the font used in the movie Cars to add as a title. I generated the text title at fontmeme.com, saved it as an image, and placed it in Tableau using the dashboard tools.\n\nNotice that throughout many of my tutorials, I often change the colors and other formatting things throughout. That’s because there is a lot that can go into the look and feel of a data visualization.\nFirstly, you can use color to highlight your insights. This is why I think I got the pie charts to work well in this scenario. I used color to highlight ONE piece of the pie. Not color for every piece ever.\nSecondly, there are accessibility reasons to consider color (and contrast) differently. There are Chrome add-ins that let you view how your data visualization would look to someone who experiences a form of colorblindness. Here are two that I use:\n\nColorblinding\nWCAG Color Contrast Checker\n\nIt’s really useful in seeing how your color choices may look to someone else. You may think your colorful rainbow data viz looks awesome, but you risk losing your audience if they can’t clearly see what you’re trying to show. That’s an unfortunate place to be for all involved.\nBut you don’t have to take my word for it. See for yourself!"
  },
  {
    "objectID": "posts/jamming-with-parameters-and-more-in-tableau/index.html",
    "href": "posts/jamming-with-parameters-and-more-in-tableau/index.html",
    "title": "Jamming with Parameters and More in Tableau",
    "section": "",
    "text": "Wouldn’t it be neat if you had a lot of data to present and wanted to give the viewer the power to explore? Parameters in Tableau can help make that happen.\nIn this post, I’ll go over how to create a “selector” with parameters that will give the viewer options to view exactly the data they want.\nIn this week’s MakeoverMonday exercise, I looked at the number of hours that men and women spent on paid and unpaid work. It was really interesting to see the differences in different countries. That being said, it would have been tough to explore that if all the data was presented at once in one data visualization.\nHere’s a tutorial on how to use parameters to present data better.\n\nCreating Parameters\nAfter I opened Tableau Public and connected the data set, I went to the worksheet along the left side to right click and Create Parameter.\n\nFrom there, I named it “Country Selector.” I also specified the data type as a “string”, selected “list,” and then added values from the Country data in the data set.\n\nWhat this did is populate all the “options” that will then go in the selector.\nNext I created a calculated field that will utilize the new parameter.\n\n\nWhat this calculated field will do is create a true/false kind of situation in a filter. If a certain country is selected via the parameter, then it will filter the data visualization for that country. I then dragged that calculated field onto the filters card to specify “true” for it to work. Other examples of this application are here and here.\nOn the dashboard itself, there is a little icon that looks like boxes. Here is where you can make the parameter “country selector” show up. From there it can be dragged on to the layout where you see fit.\n\n\n\nFormatting Options\nBack to the worksheet itself, I happened to have data for average hours spent on different activities. I dragged that onto the “label” of the marks card and then right clicked to make it show “percent difference.”\n\nI right clicked on it again to make it show relative to “previous” point. What that will do is calculate the percentage change between one point to the next.\n\nFrom there, I was able to format it to show a percent symbol after the number. There are multiple ways to access the formatting options and here is one of them.\n\n\nFor this data visualization, I had two worksheets: a map and a line graph. So for each one, I formatted the tool tips to show information more like a readable sentence.\n\n\n\n\nActions and Putting it Together\nOn the dashboard, I went to Dashboard > Actions to create actions that would make the two data visualizations interact with each other.\n\nThis particular parameter action is activated by “hovering” over the map. It will show only the line graph for that particular country and the name.\nRecall that the country selector already on the dashboard. It will show only the line graph for a particular country, so the viewer has a couple options to explore.\n\n\n\nThe Final Data Visualization\nHere is the final data visualization. It has two visuals, a “country selector,” some actions, some formatted tool tips, and some formatted text.\nAnd here is the interactive dashboard! Also, here is another example of making a selector for comparing things. That particular example happens to just use traditional filtering techniques. What’s the difference? Here’s a discussion on why you might use one or another."
  },
  {
    "objectID": "posts/jitter-plots-an-easy-way-to-make-your-insights-stand-out/index.html",
    "href": "posts/jitter-plots-an-easy-way-to-make-your-insights-stand-out/index.html",
    "title": "Jitterplots - An Easy Way to Make Your Insights Stand Out",
    "section": "",
    "text": "Jitter plots are a great way to make data more visible for actionable insight.\nAccording to Inc.com, up to 73% of company data goes unused. Part of that comes down to actually understanding what it is, how to read it, and what can be done with it.\nAlso, according to a study, presentations using visuals are 43% more persuasive than ones without visuals.\nIn addition, according to MovableInk, it takes only 13 milliseconds for the brain to process an image. That’s 60,000 times faster than it takes to process text.\nThere are a ton of stats beyond these ones that state the importance of visuals. We have a big opportunity here to communicate meaningful insights visually with our BIM data and beyond in the AEC space.\nMy goal here is to illustrate how to showcase your findings for stakeholders. One of those ways is with a jitter plot, which I’ll show how to create in a bit. But first, a bit on why you might want to create one in specific instances.\n\nThe Benefits of Data Visualization in the AEC Industry\nThis ties back to the first post I made about the benefits of data visualization in the AEC industry. Many of the dashboards I see seem to have so much information and color all over the place. That might be great for your own use but could be overwhelming to stakeholders.\nMaybe there are BIM models that are opening way too slowly. Maybe there is an excessive amount of project issues in one area you want to highlight. Or maybe, you’re trying to highlight business metrics with what kind of products/projects you are quoting. CAD managers especially might have technology business cases to make on ROI that could be shown visually for buy-in purposes. The possibilities are endless.\nThere are many techniques for “best practices” that I am learning from participating in the weekly MakeoverMonday data community exercises.\n\n\nMakeoverMonday 2020 Week 2\nWith that being said, the MakeoverMonday community is back at it again with the 2020 Week 2 data set. This data set examines the amount of pesticides that are approved in the US and not approved in other countries/regions. Again, I know it’s an unrelated data set, but the “best practice” concepts can definitely apply in multiple situations. Often times, I find inspiration from going outside of my field. That’s how I found out about MakeoverMonday in the first place.\nFor those unfamiliar with how MakeoverMonday works, it starts out with a data set, some insight, and an original visualization. For the exercise, you are to find a data story and figure out how to improve upon visualizing it. In this way, you are constantly iterating new designs in your own work and collaborating with the community. I have learned so much so far!\n\n\nThe Original Data Visualization\nIn this case, here is what the original data visualization with the data set about banned pesticides looks like:\n\n\n\nOriginal data visualization. Source: EH Journal/Biomed Central\n\n\nLooking at the original data visualization provided, I am able to assess what improvements I’d make to my own version of this data visualization.\nI like the idea of trying to show amounts in each category. However, the category ranks aren’t sorted, so my eyes bounce around everywhere to see which categories are the largest. Also, I don’t like how actual countries/regions are all listed next to manually created/combined categories. Lastly, everything is the same color in this chart. What story is this trying to tell? It’s just data and color and there is nowhere specific for me to look at.\nUsing these observations, I can then go on to create my own iteration of a visualization.\nContrary to what some may believe, great data visualization isn’t just opening up the software and poking around. It takes time to prepare data, analyze data, find a story, and then plan the visualization. Most importantly, it needs to be displayed in a way that makes sense to the audience.\n\n\nWorking in Tableau Public\nAfter I did an initial sketch and observation, I then opened up Tableau Public to create my data visualization.\nIt’s important to note that the data sets provided by the MakeoverMonday team tend to be cleaned up already. However, real life data is often messy, might have have inconsistencies, or might be incomplete.\nFor additional context, here is a snippet of the raw data. I find it helpful to show this, so you can more easily associate the non-visual aspects with visual aspects. I’m a very visual person, so this is an area I’m working on.\n\n\n\nCategory, Lbs. Pesticides Used in USA Agriculture, and % of Total\n\n\nAfter connecting in the raw data in Tableau Public, I then went to the worksheet. My initial sketch showed the data in a scatter plot, so I set out to create that in Tableau.\n\nHere is what happens when you put the category pills on colors and details part of the marks card, the sum of pesticides pill on the columns shelf, and the sum of the percentages pill on the rows shelf.\n\n\nExploring the Data\nFrom here I excluded items like “total” because it seemed redundant with all the other data there already. This helped narrow in on the story I wanted to tell. However, I started seeing that data points were still overlapping. I tried playing with shapes and size of shapes, but nothing was really making them stand alone quite yet.\n\n\n\nSizing of Shapes\n\n\nTo help the situation, I learned how to create a jitter plot with a calculated field.\nBut first: What is a jitter plot?\n\n\nJitter Plot – How To\nAccording to the Data + Science website, a jitter is a random index value that can be associated to data points. Then, when you create something such as a dot plot, the data points won’t overlap. Data points might be assigned numbers like 0, 1, 2, 3, 4 etc. to try to separate them out.\nI wasn’t really aware of this before I started the week’s Makeover Monday exercise. Here is a step by step approach of how I applied my learning.\nI first right clicked in the white space below measures to create a calculated field.\n\n\n\nCreating a calculated field\n\n\nIn the next window, I named the calculated field “Jitter,” so I could easily refer to it later. I then typed INDEX() in the box and clicked OK. It’s that simple!\n\n\n\nCalculated Field with Jitter\n\n\nFrom here, I placed this new calculated field onto the columns shelf. I then clicked the little flyout arrow to get into the options. I specified Compute Using > Category. This assigns each category an index number to help space them out.\n\n\n\nJitter Calculated Field on Columns Shelf\n\n\n\n\nDisplay Options\nAfter this, I played around with swapping rows and columns. This is what I love about exploring ways to do data visualizations. It’s often never “perfect” the first time you do something. I like to think of it like a clay pot you’re building and forming from the ground up.\nKeep in mind that the following screenshots for formatting and display aren’t in a specific order.\n\n\n\nSwap Rows and Columns\n\n\nI then shifted my focus on how to highlight certain categories for the jitter plot. All the colors in the legend show up by default, which I think we are all pretty familiar with. However, with so many colors and categories, this is where the data story can get lost. For persuading action or drawing attention to insight, I want a smaller scope to focus on. I created groups for Europe, Brazil, and China. The rest would all stay gray.\n\n\n\nCreating Groups to color them differently\n\n\n\n\nFormatting Options\nFormatting and annotating are some of my favorite parts of doing data visualizations. It’s like the final touches to really drive the points home. Exploring these options really shows that there can be a lot of redundant and unnecessary things in visuals.\nIt really is a case by case type thing. Highlight what you actually want and remove everything else that doesn’t aid in telling the data story. This is why I absolutely loved reading (and finally finishing) the MakeoverMonday book!\nHere are some things I thought as I was formatting my data visualization (in no particular order):\n\nRemove a color legend and color code names in a title instead\nAnnotate specific percentages instead of listing them all\nMake use of color/lack of color to highlight what you actually want to show\nHide headers and put context in hover tool tips or annotations\n\n\n\n\nThe colors within the title eliminate the need for a legend as it would then be redundant.\n\n\n\n\nAnnotations\nHere is often what I see when I create annotations. These items seem to show up by default. In my opinion, they don’t seem to be in easily readable language. For example, reading “25%” is a bit easier on the eyes compared to “% of total = 25.”\n\n\n\nDefault Annotation\n\n\nReformatting the annotation can help it be more readable while also showing pertinent information.\n\n\n\nBetter formatting\n\n\nThere is a multitude of ways to format and finesse a data visualization before moving it onto a dashboard.\n\n\n\nThere are countless formatting options such as colors, removing unnecessary grid lines, removing headers, and more.\n\n\n\n\nGetting it onto a Dashboard & Publishing\nHere’s a quick tip I learned from the last time I worked on a MakeoverMonday data visualization.\nI used to get frustrated over the published data visualization not looking like what I intended before publishing. My understanding is that it does this because visualizations can be made to be responsive on different devices. Also, sometimes fonts aren’t supported in every browser.\nSpecifying the dashboard as “fixed size” allows me to call out the exact size I want the visualization to be.\nDifferent situations may absolutely call for other options. I’m mainly looking for a consistent look after publishing. This little tip easily saved me 15 minutes of fighting with it and republishing it multiple times.\nHere is my “final” data visualization shown as a jitter plot that highlights some regions and acknowledges that combined categories exist. Here is the interactive dashboard! It is best viewed on either a tablet, laptop, or desktop computer.\n\n\n\n“Final” data visualization for Makeover Monday 2020 Week 2\n\n\n\n\nThe Takeaway\nCreating a jitter plot is just one of the many ways to make your data and insights stand out. Much of what I show in my posts is within the free Tableau Public app. However, I do know that other tools have similar options as well.\nTo explore this, I recently looked into creating Pivot tables in Excel as well as Microsoft Power BI. I’ve even seen these concepts apply in infographics created with Adobe Illustrator. I hope to touch base with parallels in how to accomplish the same kinds of concepts in future posts. The key takeaway is that you can provide meaningful insight for your stakeholders with various tools. Stay tuned for more posts on how I explore “best practices!”\n\nSources:\nAlexis, C. (2018, November 12). 29 Incredible Stats that Prove the Power of Visual Marketing. Retrieved January 18, 2020, from https://movableink.com/blog/29-incredible-stats-that-prove-the-power-of-visual-marketing/\n\nBarrett, J. (2018, April 12). Up to 73 Percent of Company Data Goes Unused for Analytics. Here’s How to Put It to Work. Retrieved January 18, 2020, from https://www.inc.com/jeff-barrett/misusing-data-could-be-costing-your-business-heres-how.html\n\nShaffer, J. (2014, August 27). Retrieved January 18, 2020, from https://www.dataplusscience.com/TableauJitter.html\n\nVogel, D. R., Dickson, G. W., & Lehman, J. A. (1986, June). Retrieved January 18, 2020, from http://misrc.umn.edu/workingpapers/fullpapers/1986/8611.pdf"
  },
  {
    "objectID": "posts/learning-tips-and-tricks-python/index.html",
    "href": "posts/learning-tips-and-tricks-python/index.html",
    "title": "Learning Tips & Tricks - Python",
    "section": "",
    "text": "As someone who loves to learn and read about all sorts of topics, I have intentionally placed myself in the “beginner’s spot” many times over the years. In any project I lead or any situation I find myself in at work, I firmly believe that there is always more to the story than what is on the surface. This allows me to connect seemingly separate ideas to create new ones.\nWhile a general approach like that can be great, it can also create quite a challenge where more depth is required to understand a specific topic.\nOn and off since about early 2020, I’ve been dabbling with learning Python coding. I hear over and over how it is an in demand skill and is easy to learn. Sounds simple enough, right? And yet, I find it so hard to stay engaged in just about any Python course I have started. I know of at least three I’ve tried to date, and only one has completely changed the game for me. It is not to say that any methods are bad. In fact I enjoyed certain aspects of each one I’ve tried, and I have multiple recommendations below. I find that it all comes down to the delivery style and what your goals are.\nHere are three main styles of learning I have tried and what I like about each:"
  },
  {
    "objectID": "posts/learning-tips-and-tricks-python/index.html#tips-and-tricks-to-learning-python",
    "href": "posts/learning-tips-and-tricks-python/index.html#tips-and-tricks-to-learning-python",
    "title": "Learning Tips & Tricks - Python",
    "section": "Tips and Tricks to Learning Python",
    "text": "Tips and Tricks to Learning Python\nNow that you know about the different approaches and what I liked about them, here are tips and tricks I have for learning Python.\n\n1. Set up an Organization System to Support Your Learning\nSetting up an organized Jupyter Notebook with code examples and notes has been really great for my learning.\nOverall, I organize everything in folders based on concept and topic, not by course. This allows me to consolidate learning from multiple sources and not lose things. It proves to be greatly helpful in searching for examples.\nI name all my files a certain way, so I know basically what the contents are before opening them. For instance, code snippets that involve importing or loading things are in an entire folder by themselves.\nI also try to keep the content of these files concise, so they are covering a very specific example. I keep user input code snippets in personalized outputs separate from input code snippets that may report certain conditional results based upon those inputs.\nWithin the files themselves, I add comments in key parts. That way, I know even more context on what the Python code is considering in the results.\n![](1_Naming-Conventions.png)\n\n\n2. Create a Nagging Question Each Step of the Way\nSimply completing what the coding exercise asks is indeed quick. I think it is even more satisfying to come up with a nagging question outside of what the instructor demonstrated. The nagging questions ask things like “How can I?” and “What if?”\nFor instance, I have built one “app” so far in the course I’m taking right now. This one had to do with producing an interactive map that plotted locations of volcanoes and other data about them.\nHere’s what it looks like at the time of writing:  \nTechnically, I finished that project, but I took it several steps further for my own learning with the following questions:\n\nWhat other colors can I use? Based on what I know about the use of color for data visualization, I looked for different colors to use in my Python project as an option.\nWhat if I need this in other units? I realized that my data set for this project had metric values instead of imperial to plot on a map of the US. I then looked into how to iterate through a list to convert from each metric value to an imperial one.\nWhat if I need to round a number? I realized that once I converted metric meters to imperial feet that all the extra decimal places introduced spurious accuracy. I figured out a way to round the numbers to a whole number instead.\nHow can I format with commas? I then realized that I was dealing with data that included elevations of over a thousand feet in height. I looked into introducing comma separators to show those numbers better.\nHow can I change the appearance of HTML and hyperlinks? I realized that the HTML example hyperlinked on the name of the volcano. I wanted it to link to something within a message such as “Read more here” instead.\nHow can I adjust the HTML and hyperlinks? I also realized that the HTML example hyperlinked to a search term with a singular variable. I thought two variables would make for a better result, so I learned how to do that. Example: Searching “Yellowstone” vs “Yellowstone Caldera” will get different results.\nHow do I change the appearance to customize this? For my map, I chose not to go with circle markers demonstrated but regular markers that had icons in them. I learned how to make a different icon show up as well as some caveats with things like fontawesome. A triangle with an exclamation point is far more fitting in locating volcanoes on a map, in my opinion.\nHow do I change the size? After I adjusted some formatting with the HTML, I realized that the popup box was too small. It created a scroll bar to display the information. I then looked into resizing the popup altogether.\n\n\n\n3. Make the Environment Inviting and Remove Barriers\nEveryone talks about practicing regularly, and I certainly agree. The underlying hack I have for this is to address barriers to make the environment more conducive to learning. That means that you remove as much resistance as you can for practicing or doing something. Having a browser or Python editor of your choice ready to go is a subtle yet easy way to keep learning. Having a computer turned off or having a messy desk blocking the way to your keyboard are obstacles. Those subtle obstacles make it tempting to say, “not today.” Fight it.\nA recent thing I realized was how I could customize the “themes” in the editor of my choice. The two code editors I was introduced to so far were Visual Studio Code and Atom. For years, I knew of a general “dark” or “light” theme with applications. With these code editors, there are even more options than I realized. I can make everything a tranquil dark blue with shades of green, gray, and pink for the text. It’s far more enjoyable to look at in addition to it being helpful for writing code in the first place. Everyone’s preferences are different. Find one that works for you.\n\nRemoving barriers plays out in other ways too. I mentioned that I love to read. Well, I have a much easier time reading tons of books only if they are specifically on my Kindle Paperwhite. In comparison, the physical books on my shelf are often partially finished and forgotten about. In contrast, I can finish an entire Kindle book in one sitting.\nWhy is that?\nWell, the Kindle has a subtle back lit screen, so there is no barrier for not being able to read in the dark. There are progress features not only in the time it takes to read the rest of the chapter but also the percentage left to finish the book. With features like that, there is no barrier in thinking I wouldn’t have time to squeeze in some reading. And finally, it’s ridiculously easy to track notes on my GoodReads account that is tied to my Kindle. To be able to make a remotely similar experience happen with physical books, I have to tab out each chapter with post it flags and make it a point to read during the day with great lighting.\nSo there you have it. What are your tips for learning Python or other subjects?"
  },
  {
    "objectID": "posts/license-to-simplify-why-less-is-more-in-a-data-viz/index.html",
    "href": "posts/license-to-simplify-why-less-is-more-in-a-data-viz/index.html",
    "title": "License to Simplify - Why Less is More in Data Viz",
    "section": "",
    "text": "Ever heard the phrase “less is more”? It sounds great in theory, but it can actually be pretty hard to implement when presenting data.\nWhat I have discovered from participating in MakeoverMonday for five straight weeks now is that I definitely have a creative side as well as a technical side.\nOften times though, we find that we really need to keep things more “simple,” especially perhaps in a business setting.\n\nSimple ≠ Boring\nSimple doesn’t have to mean boring though! There is something to be said about a data visualization that is minimal in terms of data ink ratio but is visually pleasing by selective use of color, chart choice, and more.\nFor this week’s challenge, I set out to make a simple yet engaging data visualization. This is a challenge for me because I’m used to incorporating a ton of detail in creative ways with pretty much everything I do.\nOkay, so let’s get into some of my process and how I approached the week’s data set. In the following context and demo, I wanted to highlight my thinking process and how the visualization evolved over time. You often see people post just the final result, but there is actually a lot of experimenting that can happen beforehand.\n\n\nContext on the Data Set\nThis week’s data set focuses on the fact that actor Daniel Craig is retiring from his role as James Bond in the movie franchise. With that being said, YouGov surveyed the opinions of Britons on who they would like the “next James Bond” to be. An interesting note is that there seems to be a difference in opinion between those in favor of leaving EU and those who are in favor of remaining in EU in Brexit.\n\n\nInitial Concept\nHere is basically what I first tried visualizing in Tableau Public. If I’m comparing categories, I make it a point to at least consider a bar chart in the beginning. Sometimes, we really don’t need to make things complicated and see what is the most straight forward way to convey information.\n\nHowever, as you can see, the Category pill and the Response pill sitting on the Rows shelf seem to show a lot of information at once. Sure if you spent time reading everything, you could see the information. But it’s not easily readable for quick insights and finding correlations because everything is presented all at once.\n\n\nExperimentation\nFrom here, I right clicked and edited the axis on a chart to specify “Reversed” to create a butterfly chart. Sometimes this involves playing with the axis on each chart or even dragging and dropping the charts around to get it to do what you want. The flexibility is awesome in Tableau. \n\nFrom here, I decided to look at swapping rows and columns to make all the bars vertical for a different look. I wanted my focus to be on comparing opinions between “leavers” and “remainers.” Maybe some people might have kept them horizontal for a “tug-o-war” kind of feel, but I just wanted to explore something different.\n\nSwapping the rows and columns seemed to somewhat go towards my goal, but the bars still seemed to be all over the place. Color didn’t seem to help much so far.\n\nSorting by descending values didn’t really help either because the categories switch around in different orders on the same chart, which could be a little unintentionally deceiving to the viewer.\n\n\n\nNew Approach\nI then realized that I think I should try multiple charts and then space them out on a dashboard. This would accomplish a few things for me:\n\nConsistent order of categories (future characteristics of Bond, James Bond)\nClear colors to showcase responses\nClear distinction and separation between “leavers” & “remainers”\n\nIn order for me to make multiple little charts, I made all the “style adjustments” I wanted in one chart. I then duplicated the worksheet over and over. Lastly, I put the Category pill on the filters card to filter the respective categories. On the respective worksheets, I specified other categories instead. \nIn the midst of doing this, I explored colors again as well as reference lines. The space between the bars really seemed to work for me, so I decided to keep that.\n\n\n\nTool Tips\nI explored tool tips to customize the insight a bit. Later on, I reformatted the numbers to show as actual percentages. I had to re-edit the tool tip to make sure it read correctly.\nKeep in mind with these charts that your changes apply per chart. After creating a tool tip for the top set of bars, I needed to create another one for the bottom. The same thing applies for reference lines, colors, marks, and more. If I didn’t have the “butterfly” chart situation going on, then I wouldn’t have had to have the extra consideration. I just wanted to point this out, so you don’t find the same initial frustration as me. \nIn this case, I mostly copied and pasted but swapped out SUM(Leave) with SUM(Remain) in the other tool tip.\n\nAt this point, I duplicated all the worksheets and filtered for each category I wanted to examine. However, once I dragged and dropped the sheets side by side on the dashboard, they lined up horribly.\n\n\nDashboard Disaster?\n\nOne of the neat parts about Tableau Dashboards is that you can layout things in all sorts of ways. What I did here was hide all the titles that came in automatically from the worksheets themselves. I then dragged and dropped “blank” spaces and then “text” from the bottom left corner of the dashboard. I mostly did this with the “tiled” option, not the “floating” option. This helped all of the charts to stay in line.\n\nHere’s an example of one of the text boxes that I ended up orienting sideways. \nAs you can see, at this point I switched the marks to show as percentages. Here’s a quick note on how I did that:\nThe “pills” (fields) are located along the left side of each worksheet where you can specify the default format.\nI right clicked on the pills that dealt with Leaving & Remaining and then specified the following to make it show up as a percentage. Note that I didn’t pick Percentage itself.\n\nThe data source format makes a difference. When I picked out Percentage, it took 73 to mean 7300% which is clearly not what I intended. For this kind of situation, a custom number format seems to accomplish what I want pretty quickly. I may revisit this to figure out other ways to address this.\n\n\nThe Final Result\nHere is what I came up with for a “final result.”\n\n\nOpted for a gold/warm gray color scheme to fit closely with the movie franchise look and feel.\nUsed gold to highlight the “acceptable” responses as opposed to highlighting “unacceptable” or “don’t know” responses.\nKept the same order of responses across the dashboard (acceptable – unacceptable – don’t know) for each category\nUsed the white space and separation between the bars to emphasize the diverging opinions of “leavers” vs. “remainers.”\n\nAnd here is the link to the interactive data visualization! It looks best when viewing from desktop or tablet.\nIf I were to revisit this again, I might eliminate the legend and use the colors to highlight specific words. Then again, gold might be hard to read unless it is on a dark background. As you can see, there are many decisions to make.\nThe main takeaway is that simplifying your presentation will allow the viewer to not be overwhelmed with data.\nThanks for reading. Now it’s time for a martini. Shaken, not stirred. ;)"
  },
  {
    "objectID": "posts/lollipop-charts-a-sweet-data-viz-technique/index.html",
    "href": "posts/lollipop-charts-a-sweet-data-viz-technique/index.html",
    "title": "Lollipop Charts - A Sweet Data Viz Technique",
    "section": "",
    "text": "Have you ever combined multiple seemingly unrelated interests into one project? I personally love it when I can do that. Today’s inspiration combines watercolor paint, Photoshop, and Tableau Public. How sweet!\nIn the MakeoverMonday #Datafam community, the 2020 Week 3 data set explores the amount of sugar that UK residents consume across different age groups.\nUpon seeing this data set, I immediately thought to approach it creatively because these weekly data exercises are meant to be educational…. and fun!\nThe topic of this week’s practice focused on sugar consumption, so it provided a sweet opportunity to try not only custom shapes in Tableau but also a neat chart technique.\nWhat I will show next may not be “best practices” for the corporate world, but it sure was fun to create.\nFollow along and learn how to use custom shapes and create what is called a Lollipop chart!\n\nWatercolor Painting\nI first pulled out my leftover watercolor paint supplies from one of my projects from Let’s Make Art and started sketching a doughnut with a bite taken out of it.\nFrom here, I started pouring out the watercolor paints and planning out the colors I wanted to make the doughnut.\n\nJust a quick note here about these paints. Let’s Make Art makes watercolor painting such a wonderfully portable hobby. The little paints take barely a minute to set up! I highly recommend it if you’re looking for an easy to set up creative outlet! I keep all my supplies (including the tray) all within my desktop monitor stands.\n\n\nPhotoshop\nI then took a picture of the completed doughnut and brought it into Photoshop to clean it up.\n\n\nCutting out the background of the completed watercolor doughnut picture with Photoshop. The image was saved as a .png to keep the background transparent.\nNow that the doughnut is ready to go, let’s see how this goes into the data visualization!\n\n\nTableau Custom Shapes\nTableau happens to have a folder directory called “My Tableau Repository,” and this is where you can place custom images into the Shapes folder. As you can see, there are already a lot of images that are meant for best practices often in business settings.\nHowever, today I decided that it is all about creativity and fun. I created a “Custom” folder and yep, you betcha I copied and pasted my doughnut.png file there!\n\nOnce I connected the source data to my Tableau workbook, I set out to incorporate the custom shapes in the software. The “original” set up data visualization was a bar chart, so I changed it to “Shapes” on the marks card. I then clicked on Shapes>More Shapes>Reload Shapes to get my new custom image to show up.\nYou’ll recognize that the list follows the Tableau Repository folder structure.\n\n\nFor the purpose of my data visualization, I decided to focus on the time period from 2014-2016 for all age groups. Others were included in the original data set.\n\nDoughnut shape chart with 2014-2016 percentages on the columns shelf, free sugar intake on the rows shelf, and free sugar intake on the filters card to filter out the other age group categories.\nFrom here I adjusted the cell size to make my doughnuts much larger on the chart. Originally, they came in much smaller.\n\nAs I was creating this data visualization, I originally wanted to have the doughnuts themselves create bars across the screen for each category. Then I realized this would look way too busy, so I looked into what is called a Lollipop chart instead. I might revisit that at a later date.\n\n\nLollipop Chart\nFirst, I placed the same 2014-2016 pill on the columns shelf again, so I got two side by side charts. I then right clicked on the second chart axis to create a Dual Axis chart.\n\nSo what this did at first was essentially make it look like one of the charts disappeared. In fact, it did not, and you can tell by the circled double axis headers below. I then was able to click on one of the headers and go to the marks card to switch it to a bar chart.\n\nFrom here I right clicked on the axis and specified “Synchronize Axis.” What this does is take the bars in the bar chart and stops them at the centers of the doughnuts instead of going past them.  From here, I adjusted the size of the bars and then started formatting them as a different color. I made them much lighter and opaque so they didn’t detract from the doughnuts themselves. \n\n\n\nAdding a Reference Line\nThe original data set talked about the recommended amount of calories that come from sugar. It’s only 5% of the day’s calories, which is clearly much less than what these actual percentages actually are across the different age groups. I decided to show this significant discrepancy in the form of a reference line.\n I right clicked on the axis and clicked on Add Reference Line. There are various settings within this to get it looking the way you want. I made mine do a dashed line and show hover text about the recommended amount of sugar. \n\n\nFormatting\nFrom here, I started changing some formatting of the data to display better. I noticed that the percentages wanted to show up with several decimal places with no discernible purpose, so I set out to reformat them.\nI right clicked on the 2014-2016 pill within Measures to go into Default Properties>Number Format. What this does is set the formatting across the board in this workbook. I could have changed the formatting in other places as well, but those seem to affect only specific instances and not affect the project “globally.”\n\nFor some reason, whatever I did initially seemed to result in percentages that were not in the correct format. So, here are the settings that resulted in the percentages with one decimal place in this case.\n From here, I got into formatting the tool tips. If you recall from my last post, the annotations and tool tips do not seem to be easily readable by default. I like to change them to read more like sentences. Here is an example of that.\n\n\n\nThe Result\nHere is what the “final” lollipop data visualization looks like with custom shapes. The interactive version with hover tool tips is located here!"
  },
  {
    "objectID": "posts/obsidian-adventures/index.html",
    "href": "posts/obsidian-adventures/index.html",
    "title": "I deleted my browser bookmarks and emails. I do this instead.",
    "section": "",
    "text": "Over the last few years, I have been finding ways to combat feelings of overwhelm. Since completing the STEP Program, I learned that systems and processes can immensely help tame the overwhelm (Perry, 2022).\nFor example, I was one of those who had thousands of unread emails with no rhyme or reason to keep them. I emailed myself links to read later, signed up for newsletters, and more. It came with feelings of anxiety and fear of missing out.\nI then came up with three essential levels of systems to help tame it.\n\nLevel 1 System:\nI reconsidered whether or not I wanted certain newsletters. If I had a habit of not opening or reading them, I unsubscribed from them. This was a hard reality to face, and it is one of the easiest ways to cut down on overwhelm. But we can take it further than that.\n\n\nLevel 2 System:\nI created email rules to automatically move emails into designated folders for more focused processing. For example, if there were coupons that came in newsletters, I used email rules to automatically move those emails to a folder called “Advertisements.” The logic is that good marketing includes catchy subject lines such as “Deal Ends Tonight!” If I saw the messages much later than the perceived urgent timeline, I knew I could delete them en masse without any careful selecting of messages in the folder. And if I caught a deal on time, it was much easier to see.\nAnother example is to use email rules to move emails to folder based on topic such as “Art” or “Coding.” If I have a topic of interest that I know has a lot of rabbit holes and can be perused at my leisure, I will use email rules to strategically sort those emails to a place where I can go when I have more time to look into them. The key is to not stop there. This helps with the Level 3 System I will describe next.\n\n\nLevel 3 System:\nThe Level 3 System involves processing any sort of input (including emails) and incorporating them into a Personal Knowledge Management System.\nBut first, what is a Personal Knowledge Management System?\nWikipedia states that Personal Knowledge Management (PKM) occurs when someone is collecting information to then classify, store, search, retrieve and share as knowledge. Here is a great video that explains it as a way we make sense of the world (Personal knowledge management 2022)(Linking Your Thinking, 2021).\nNow that we have a basic understanding of the concept, let me introduce you to how I am learning how to set up a Personal Knowledge Management system using Instapaper, Obsidian, and Dropbox.\n\n\nMindset Shift\nPrior to the first step in setting up a PKM system involved a mindset shift in how I wanted to change things. Without a mindset shift, I risked not successfully implementing the change I wanted to see. For this effort, I recognized that I did not like having piles of emails, forgotten favorites, and unorganized bookmarks. The “gems” I forwarded to myself via email to look into and act upon were unfortunately lost in the noise. I did not like that.\nI then crafted a plan for a future state.\nThose who are on a quest to find better ways to capture thoughts and remember things may or may not be familiar with concepts such as a “Brain Dump,” a “Second Brain,” or perhaps even a “Mind Palace.”\nI like to think that setting up a system (“Second Brain”) will allow for getting thoughts out of my mind on a regular basis (“Brain Dump”). It also could create an opportunity to relieve stress and remember things better from spatial associations (“Mind Palace”). This was the future state I wanted. (Forte, 2022) (Mathers, 2022)(Mind palace memory technique 2020).\nI came up with a series of “Ground Rules” to implement this future state for me. The first few “Ground Rules” designated browser bookmarks and favorites with a much tighter definition:\n\nOnly login shortcuts and tools are allowed to be browser bookmarks.\nOnly general blog feeds are allowed to be “favorites” if I wanted content but did not want to subscribe to a particular newsletter.\nArticles, blog posts, tips, and other “informational posts” are not allowed to be browser bookmarks or “favorites.”\n\nI also created tighter rules around emails, which I will come back to in a minute.\n\n\nSet up the System\nI downloaded and installed Obsidian. I connected it to a folder on Dropbox to allow for backups. I picked out a theme I liked as well as a couple plug-ins that highlight code syntax and create visual mind maps of notes. Beyond that, I stopped customizing at that time and moved on to the heavy-lifting of the system setup.\n\n\n\nObsidian plug-ins\n\n\n\n\nBuilding the System Workflow\nOnce I decided upon reasonable “Ground Rules” for emails and bookmarks, I went to work. I started a new folder in Obsidian for each topic I was interested in. I then started new notes in each folder.\nLooking at one bookmark link at a time, I implemented a quick processing routine:\n\nSkim the article\nLink to the article in an Obsidian note\nUse bullet points and other markdown techniques for quick takeaways\n\n\n\n\n\n\n\nImportant\n\n\n\nDo not take notes throughout reading. Only record the few things you may implement as action items.\n\n\n\nLiberally use tags and backlinks to link to other notes because inevitably there will be connections while doing this process\nDelete the bookmark link from the web browser forever. Yep.\n\nI repeated this routine for upwards of 40 times, quickly deleting links left and right. Pretty soon, I was left with a couple key shortcut logins and some shortcuts for work.\nI did a very similar routine with my emails. As I mentioned earlier, good email marketing will draw you in with catchy subject lines. I relied on that concept to either delete the unfortunately uninteresting email newsletters immediately or follow a similar routine that I had for the bookmarks.\nThe fear of missing out had an initial hit but faded throughout the process. When it comes down to it, would you rather have so many emails captured that you never read, or would you want to create something extra special out of the few things you did read?\nBy this point, I had several folders and notes captured within Obsidian. I started seeing connections between various things I wanted to try and articles I read. The mind map visual from an Obsidian plug-in is a nice touch. As I took notes, I realized that I was creating action items. The action items that had two or more connections resulted in some neat projects I really want to build. None of this would have happened if these things were still sitting in my emails and bookmarks.\n\n\n\nmind map plug-in\n\n\n\n\nSustaining the System\nHere is where Instapaper comes into play.\nAfter signing up for an account, I made sure to make it an available option to “Share” links to.\nOn my phone I am able to take links and “Share” them to Instapaper and move on with my day. What that does is create a “queue” where I can read articles and then process into takeaways for my Obsidian notes. Once I’m done reading an article on Instapaper, I archive it.\nFinally… great ideas are no longer sent to a forgotten pit of doom in my email inbox.\n\n\nKey Takeaways\nThis was a high-level overview of the how and why with regard to how I am learning to set up a Personal Management System. I state that I am learning because tomorrow I may implement a tweak in my systems upon new information and understanding. And to end this blog post, that was the whole point all along.\n--\nSources Cited\nForte, T. (2022, September 14). 12 steps to build a second brain. Forte Labs. Retrieved December 12, 2022, from https://fortelabs.com/blog/12-steps-to-build-a-second-brain/\nLinking Your Thinking. (2021, April 19). What is PKM? what is personal knowledge management? YouTube. Retrieved December 12, 2022, from https://youtu.be/Q2WBHyqRsxA\nMathers, C. (2022, March 9). 5 steps to do a brain dump (with templates). Develop Good Habits. Retrieved December 12, 2022, from https://www.developgoodhabits.com/brain-dump/\nMind palace memory technique. Memorise. (2020, September 13). Retrieved December 12, 2022, from https://memorise.org/memory-training/mind-palace-memory-technique\nPerry, A. (2022, December 6). Learn do become. Learn Do Become. Retrieved December 12, 2022, from https://learndobecome.com/\nWikimedia Foundation. (2022, September 24). Personal knowledge management. Wikipedia. Retrieved December 12, 2022, from https://en.wikipedia.org/wiki/Personal_knowledge_management"
  },
  {
    "objectID": "posts/personal-project-spotlight-constructing-new-york/index.html",
    "href": "posts/personal-project-spotlight-constructing-new-york/index.html",
    "title": "Personal Project Spotlight - Constructing New York",
    "section": "",
    "text": "The thing I seemed to notice about the MakeoverMonday data visualization learning exercises is that they seem to deal with fairly clean data and supporting articles right there for context. Also, everyone else is working on them too, so if you’re stumped, you can just hop on Twitter and learn how others are fairing in the challenge. It’s almost like everything just kind of works out, and you’re on a nice Sunday drive on a winding road. Maybe a couple hiccups here and there if you’re ambitious, but that’s about it. Oh how nice.\nThis week I tried something different before getting to the usual weekly exercise, and wow, that is not a walk in the park at all. I decided to pursue a personal project.\nI thought it would be neat to see how buildings came about over time, and what better place to see something like that than New York. I’ve personally been to NYC myself a couple times and there are so many buildings!\nI decided to structure this post a bit differently with “lessons” learned. That way, you’ll see what kind of challenges can exist in data. No white picket fence step-by-step tutorial is going to prepare you for every situation. It’s important to keep trying and applying tools from the tool box.\nSo what I am about to show you here is something I’m really happy about accomplishing after a ton of troubleshooting!\n\nLesson #1: Dealing with dates is just the worst sometimes\nI think just about everyone in the corporate world has had the experience of formatting dates in Excel. Sometimes you format the data by just the year, or maybe the month and day. Maybe you need time stamps or adjust for time zones too. And sometimes, you even need to format data to not be displayed as a date. Ever had a fraction get formatted to Jan-2?\nWithin Tableau, these kinds of nuances do indeed exist. I found a great blog post on how to handle them within Tableau, so they are displayed properly.\nHowever, I also found a challenge in dealing with calculated fields.\nUsually, when I illustrate calculated fields in my blog posts, it’s just as simple as IF [field] = “9” THEN yes … meaning, if the number 9 shows up in a certain field, then tell me yes!\nWell, with dates, the syntax is a bit more complex.\nThe data set I had included the year that the building was built. It also included a possible two points in time after that where the building might have been altered. I thought that was interesting information. However, not every building was altered multiple times. “Null” values showed up in a lot of places, and I didn’t like that. Filtering out “null” values made things worse because then it excluded the entire row of data.\nInstead, I wanted to have it display “N/A” instead of “null.” Applying the whole “if this then that” idea wasn’t as straightforward here. Simply putting in IF [YearAfter1] = “Null” then N/A just caused errors because the [YearAfter1] field is formatted as a date. “Null” isn’t a date. If you try to make a conditional statement with two different types of data, you’re gonna have a bad time.\nThe calculated field here shows that the field [YearAfter1] is being formatted to be displayed by a year (date part). Then, once it’s converted to a year, turn that into a string briefly (str). Now, if the string “null” exists in the [YearAfter1] field, it will then be replaced with the string “N/A.”\nThis is definitely an area that I need to keep exploring, but at least I got this one to work! There is something so satisfying about seeing “The calculation is valid” after 50,320,409 times that it said it contained errors in red text.\n\n\n\nLesson #2: Using fancy background maps sometimes feels like hanging a picture frame without a level\nTableau does come with decent default world maps if you have geographical data like zip codes, etc. It tends to generate longitude and latitude fields to help plot where the points go. Custom maps are also an option.\nUsually, many tutorials talk about locating the area on the custom map, downloading an image of it, and then mapping it in Tableau via X and Y coordinates.\nI found this to be a challenge because whatever X and Y coordinates that came with the data set I had looked differently. They weren’t really the same format as any latitude and longitude numbersI saw on the map I exported. I honestly have no clue why. It wasn’t as simple as just typing in the X and Y that matched with the latitude and longitude numbers I found with the map. If I did that, then the picture of the map would have been crazy small.\n\n\n\nTo make it extra challenging, apparently the map background that came in at scale wasn’t aligning to my data. The data looked skewed when it started plotting points. What I ended up doing was trying to square up the X and Y axes.\n\n\nAfter that, I specified the default Background Maps to “none.” This would allow the custom map image to show up instead. I called it a night after that.\n\n\n\nLesson #3: Duplicates and weird stuff can happen in data\nIn the data set I was working with, I saw some odd things.\n\nA building that was apparently built in the future (2040)\nMultiple accounts of Rockefeller Center being built in different years\nThe Park Row Building stated 29 floors, but other sources noted storeys and floors in varying ways\nCut off names in landmark names, etc.\n\nThere are times where you can exclude items like these or you can note that there is a gap. What is important is that you don’t skew what is going on if you pick either of those options.\nIn my case, I decided to hide the 2040 building because it didn’t seem to detract from or skew the message of “growth over time.”\nFor the Rockefeller Center duplicates, I decided to research a bit further with other sources to narrow down duplicates. From there, I created a set in Tableau called “Keeper.” I then added all the unique landmarks to it. Simply excluding the duplicates outright seemed to hide all of the Rockefeller entries, so creating a set seemed to work better.\n\nFor the other items I saw, I mainly just let it go. In the case of floors vs storeys, there seems to be a debate on this. My whole point in illustrating it altogether was showing some distinct differences between most buildings and the first skyscrapers.\n\n\nLesson #4: Different fonts in Tableau can be tricky, but there is a workaround\nEarly on in my “data viz adventures” I would create my nice little data visualization, pick out fonts, colors, etc. As soon as I hit publish in Tableau Public, I would see that all my fonts would change to something else that was perhaps more “web” friendly. This also would screw up the layout.\nWhat I’ve learned over time is to create dashboards that are “fixed” in size. I also learned to have images representing fonts that are different from the norm. That’s just what I personally do; I’m sure there are lots of methods.\nWhat that means is that I’m adding a .png image of the text I want instead. This keeps the “style” I want intact.\nTo do this, I found afont generator online. I can just type what I want, pick out a font, and then download the .png image with a transparent background.\n\nFrom there I can add my “title image” via the tools in the bottom left of the dashboard.\n\nI decided to make my data viz look like a New York Times newspaper. For added flair, I added borders to the text boxes and “blanks.”\n\n\n\nLesson #5: Creating groups can help manage the chaos\nI thought it would be neat to display the landmarks built in time periods. The viewer could then pick what they wanted to see. The problem was that parameters only allowed for single selections. I needed a “one to many” type situation on the fly.\nWhat I did in this case was a create group.\nTo create this group, I right clicked on the [Year Built] field and specified create group. From there, I grouped the dates together manually.\n\n\nAfter I grouped together the different time periods, I created a parameter that would then list all these time periods. These time periods would then be my “options” for the viewer to choose on the fly.\n\n\nTo make that “parameter selector” actually filter the data based upon user selection, I quickly created a calculated field.\n\nI then placed the Time Period_Decade Show calculated field on the filters card. I also displayed the Time Period_Decade Selector parameter on the dashboard itself. These two working together would then display the correct landmarks per the time period, much like my previous tutorial posts.\n\n\nThe Final Viz and the Takeaway\nHere is what I came up with after working on this data visualization on and off for about four days.\n\nI find it so fascinating that there was a huge construction boom in the early 1900s. It’s easy to see that because of the animations I built into this Tableau dashboard.\n\nAlso, the bar chart with the number of floors shows a nice comparison with the first sky scrapers. What I found to be the most interesting out of the whole analysis was the landmarks themselves. I didn’t know the oldest mansion in that area was build in 1765, which was before New York even became a state. There are even older buildings than that in the neighboring boroughs as well. I certainly learned some really interesting trivia about the Manhattan borough while visualizing this data.\nOverall, I am really happy with what I accomplished with this data set. I kept trying over multiple days to accomplish what I envisioned. I’m glad I didn’t give up or decide to do something “easier.” Now I am that much stronger in addressing the same issues in the future. MakeoverMonday will always be a great practice I’d like to continue. However, I think every once in a while I really need to venture out.\nI had no “supporting article” handed to me here. I didn’t have all the data I wanted all up front. This was just me, a bunch of data, and my tools in my toolbox I built over time. I feel that I grew and learned a lot from taking on a personal project. I feel really satisfied and hope to inspire you to do the same.\nSources:\nCity of New York. (n.d.). Retrieved May 2, 2020, from https://www1.nyc.gov/\nNew York City – Buildings Database. (2017, September 1). Retrieved May 2, 2020, from https://www.kaggle.com/new-york-city/nyc-buildings"
  },
  {
    "objectID": "posts/picture-perfect-penguins-dynamic-tableau-shapes/index.html",
    "href": "posts/picture-perfect-penguins-dynamic-tableau-shapes/index.html",
    "title": "Picture Perfect Penguins - Dynamic Tableau Shapes",
    "section": "",
    "text": "Data visualizations are great, but what if you have a viewer who might not be as familiar with the subject matter? Well, guess what. Tableau can help! By using shapes in Tableau, you can import actual pictures into your dashboard and make them change dynamically based on what you are trying to show the viewer.\nTo showcase this, check out this analysis on penguins. The pictures of three different penguins can change based on what the viewer picks out from a drop down. I did this because simply reading text saying “bill length,” “bill depth,” and “flipper length” kind of forces the viewer to have to visualize what this penguin looks like compared to the other types. By filtering the pictures and the data for exactly what the viewer picks out, the viewer can get a much better idea of what it all means.\nWant to learn how to build it? Read on!\n\nPicture Perfect Penguin Tutorial\nTo begin, we need to create the first of four scatter plots. You’ll see below here that I have many worksheets in this workbook. That’s because most of them are scatter plots comparing different attributes of the penguins.\nThe main idea here is two put two of the comparison attributes on the columns and rows shelves and then color code by species. In the screenshot here, I compared Average Flipper Length and Average Body Mass. The “species” field was placed on the color of the marks card.\n\nNote that all the other fields on the marks card are mainly meant for placing within tool tips for later.\n\n\nPenguin Viewer\nFrom there I created a new worksheet tab and called it Penguin Viewer. I clicked on the Shapes on the marks card and then went to More Shapes.\n\nFrom there I entered a magical place, the Tableau Repository. It tends to be located here: C:\\Users\\USERNAME\\Documents\\My Tableau Repository\nWhatever you place in the repository folder structure then shows up as a Shape that can be used in Tableau. What I did here was locate some actual pictures of specific penguins from Wikipedia and placed them in a “Custom” folder I placed in the repository.\n\nAs you can see here, the Species field was placed on the Shapes on the marks card. Therefore, when I click on the Shapes button itself, I can assign it a “shape” or specific penguin image.\n\nOnce I did that, I made the size of the shapes a bit larger so all three penguin species could be seen side by side.\n\n\n\nPenguin Selector\nNext, I built a “penguin selector,” which in this case is really just a filter. I did it as a filter as opposed to a parameter this time because it was the easiest way that I could offer singular options as well as an “All” option. This will filter for the correct penguin picture as well as all the corresponding scatter plots that go with it.\n\nNote that the Species field was placed on the filter shelf. There is also a symbol next to it. Hovering over it reveals a drop down menu.\n\nI selected it and then specified “Apply to Worksheets.” What that does is let me pick out specific worksheets where this filter applies.\n\nI specified here that I wanted this filter to apply everywhere but the “Species Studies at Each Island.” I did that because that particular worksheet was a bar chart, and I didn’t want it filtering to just a single bar. It’s not exactly insightful in my opinion to do that, so I unchecked that box.\n\nDoing this eliminates the need for an Action in Tableau to filter for me, so that’s slick. From there, I formatted and designed the dashboard to follow the same color scheme as the Palmer Station penguin logo.\n\n\nThe Final Data Viz\nHere is how the final data visualization turned out. The filter was placed on there for the viewer to select. If nothing is selected, it defaults to “All” for all visualizations. Once a different selection is made, all the visualizations (except for the one in the top right) filter for the attributes about a specific penguin type.\n\nHere is the interactive data visualization. Also, here’s a video of how it works."
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html",
    "href": "posts/posit-2022-table-contest-entry/index.html",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "",
    "text": "I recently came across the Posit 2022 Table Contest and decided to participate for the first time. I am relatively newer to {R} but not new to data visualization, so I thought it would be a great opportunity to expand upon my learning. To go along with the official requirements, the code repository for my entry is located on GitHub, and I have classified my entry as a hybrid “Other” submission type because I consider it a hybrid high-level narrative on chart/table design + mini tutorial on some key elements.\nFor my entry type, I will walk through the high-level considerations I had in creating tables in {R} and line plots in {Python}. My data topic of choice is Construction Spending in the U.S. as it is relevant to the industry I work in."
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#data-source-considerations",
    "href": "posts/posit-2022-table-contest-entry/index.html#data-source-considerations",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Data Source Considerations",
    "text": "Data Source Considerations\nThe data sources for Construction Spending in the U.S. are ever changing as the survey is conducted with monthly reports. In addition to that, the Excel reports that are typically available include a span of only a few months at any given time.\nFor the Posit Table Contest, I merged three different data sets, so I could get an entire year’s worth of data. All data sets were brought into my code via URL links as opposed to static files.\nHere is how I imported the data sets and prepped them for use with {R}. I grabbed seasonally adjusted data from the first tab of three data sources (most recent and two further back). I skipped the first four rows to go beyond any “header” information in the tables.\n\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(dplyr)\nlibrary(sparkline)\n\n\n# import data - June 2022 - Oct 2022\nurl <- 'https://www.census.gov/construction/c30/xls/release.xlsx'\n#url <- 'release.xlsx' #alternatively, please use the provided spreadsheets on GitHub in a static way\nurl1 <- rio::import(file = url,which = 1, skip = 4)\n\n\n# clean up data\ndf1 <- url1 %>% drop_na()\n\n\n# defining column names using vector\nnames(df1) = c(\"Type_of_Construction\",\"Oct_2022\",\"Sep_2022\",\"Aug_2022\",\"Jul_2022\",\"Jun_2022\",\"Oct_2021\",\"Ref_Sep_2022\", \"Ref_Oct_2021\")\n\ndf1 <- select(df1, Type_of_Construction, Jun_2022, Jul_2022, Aug_2022, Sep_2022, Oct_2022, Oct_2021, Ref_Sep_2022, Ref_Oct_2021)\n\n###\n\n\n# Jan 2022 - May 2022 data\n# import data from URL\nurl <- 'https://www.census.gov/construction/c30/xls/pr202205.xlsx'\n\n#url <- 'pr202205.xlsx' #alternatively, please use the provided spreadsheets on GitHub in a static way\nurl2 <- rio::import(file = url,which = 1, skip = 4)\n\n# clean up data\ndf2 <- url2 %>% drop_na()\n\n# defining column names using vector\nnames(df2) = c(\"Type_of_Construction1\",\"May_2022\",\"Apr_2022\",\"Mar_2022\",\"Feb_2022\",\"Jan_2022\",\"May_2021\",\"Ref_April_2022\", \"Ref_May_2021\")\n\n#selecting specific data (not June as it's already captured in df1)\ndf2 <- select(df2, Type_of_Construction1, Jan_2022, Feb_2022, Mar_2022, Apr_2022, May_2022)\n\n\n# Nov 2021 - Dec 2021 data\n# import data from URL\nurl <- 'https://www.census.gov/construction/c30/xls/pr202201.xlsx'\n\n#url <- 'pr202201.xlsx' #alternatively, please use the provided spreadsheets on GitHub in a static way\n\nurl3 <- rio::import(file = url,which = 1, skip = 4)\n\n# clean up data\ndf3 <- url3 %>% drop_na()\n\n# defining column names using vector\nnames(df3) = c(\"Type_of_Construction2\",\"Jan_2022\",\"Dec_2021\",\"Nov_2021\",\"Oct_2021\",\"Sep_2021\",\"Jan_2021\",\"Ref_Dec_2021\", \"Ref_Jan_2021\")\n\n#selecting specific data (not Oct as it's already captured in df1)\ndf3 <- select(df3, Type_of_Construction2, Nov_2021, Dec_2021)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe latest release gets updated approximately every month as new survey results come out from the U.S. Census Bureau. Because of this, the tables and charts will update if the code is run again. However, special attention will need to be given to updating the column names, static annotations, and data sets that get merged over time. As an alternative, please use the provided spreadsheets on GitHub for exact reproduction at any time after the date of this publishing."
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#combining-data",
    "href": "posts/posit-2022-table-contest-entry/index.html#combining-data",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Combining Data",
    "text": "Combining Data\nAfter selecting and ordering the columns for the three separate data sets, I combined them into one data set. I did it like this as opposed to any other way because the formatting and structure was the same in each data set. Simply placing them side by side and then selecting the exact columns I wanted created the master data frame in the correct order (oldest to newest time series).\n\n# merge all dataframes\ndf <- cbind(df1, df2, df3)\n\ndf <- select(df,Type_of_Construction, Oct_2021, Nov_2021, Dec_2021, Jan_2022, Feb_2022, Mar_2022, Apr_2022, May_2022, Jun_2022, Jul_2022, Aug_2022, Sep_2022, Oct_2022, Ref_Sep_2022, Ref_Oct_2021)"
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#segmenting-the-data",
    "href": "posts/posit-2022-table-contest-entry/index.html#segmenting-the-data",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Segmenting the Data",
    "text": "Segmenting the Data\nAnother special consideration in creating these tables involved dealing with multiple overall categories of construction types in the same table. I segmented the master data frame into three main segment data frames to represent “Total Construction,” “Private Sector,” and “Public Sector.” These three data frames were then used to create three tables for the Posit Table Contest.\n\n# segments dataframes by construction category\ndf_all <- head(df, 19)\ndf_private <- df[20:35, ]\ndf_public <- df[36:50, ]"
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#table-considerations",
    "href": "posts/posit-2022-table-contest-entry/index.html#table-considerations",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Table Considerations",
    "text": "Table Considerations\nI explored {gt}, {gtExtras}, {tidyverse}, and other packages to create the tables.\nMy current understanding is that the syntax for these tables sort of works like opaque paint in that different layers are applied in a certain order to get a certain effect. A specific example of this is where I applied special formatting on the “overall summary” categories such as “Nonresidential,” “Residential,” and anything with “Total” in the name to differentiate them a bit from the sub-categories. I have the gt_highlight_cols part before the gt_highlight_rows part, which results in a lighter color for the “overall summary categories” and a slightly darker color for the sub-categories.\nThe sparklines were particularly a challenge to create because these data sets have multiple columns of data over time per construction type, and I wanted each sparkline at the edge of the tables to look like time series charts.\nWhat helped me accomplish that was this part of the code where it “mutated” the columns to be read like rows with the exception of a few columns. The “trend” column displays the sparklines.\n\nfinal_df_all <- \n  df_all %>%\n  rowwise() %>%\n  mutate(trend = list(c_across(-one_of(\"Type_of_Construction\", \"Ref_Sep_2022\", \"Ref_Oct_2021\")))) %>%\n  select(Type_of_Construction, Oct_2022, Oct_2021, Ref_Sep_2022, Ref_Oct_2021, trend)\n\nOne of my last big considerations in the table design was all about color. I wanted to be mindful of color with respect to those who may experience forms of color blindness, so I went to Adobe Color for all of my color palette work. I then applied those colors to specific situations. In this case, I made all the negative percentages show up as a light red to contrast with the positive percentages that were light blue. I also have markers that result in upward and downward pointing triangles to go along with the positive and negative lines.\nHere is the full code that produces one table for All Construction. The other two tables are similar. In the order of execution, this code does the following:\n\nMutates columns as rows to get the sparklines working\nSelects columns of data to display for the table\nSets the theme\nSets the title and subtitle\nColors/fills in cells conditionally\nAdds additional formatting for overall categories vs. sub-categories\nAdds dividers to give more order to the table\nFormats values such as percentages and currency\nCreates the sparklines with specific styles at the edge of the table\nAdds text above a couple columns to group them in a related understanding for the viewer.\nAdds footnotes\nControls spacing of cells in a consistent way\n\n\n# Table - All Construction\nfinal_df_all <- \n  df_all %>%\n  rowwise() %>%\n  mutate(trend = list(c_across(-one_of(\"Type_of_Construction\", \"Ref_Sep_2022\", \"Ref_Oct_2021\")))) %>%\n  select(Type_of_Construction, Oct_2022, Oct_2021, Ref_Sep_2022, Ref_Oct_2021, trend) %>%\n  gt() %>%\n  gt_theme_nytimes() %>%\n  tab_header(\n    title = md(\"**Construction Spending - All**\"),\n    subtitle = md(\"*Data presented in millions of dollars. Rounding may affect accuracy when summarizing data.*\")\n    ) %>% \n  tab_options(heading.align = \"left\",\n              table.border.top.color = \"white\",\n              table.border.top.width = px(3)) %>%\n  tab_style(\n    style = list(\n      cell_fill(color = \"#D9E8F5\"),\n      cell_text(color = \"#304269\"),\n      cell_text(weight = \"normal\")\n      ),\n    locations = cells_body(rows = Oct_2022 > Oct_2021)\n  ) %>%    \n  tab_style(\n    style = list(\n      cell_fill(color = \"#F58BA1\"),\n      cell_text(style = \"italic\"),\n      cell_text(color = \"#590219\"),\n      cell_text(weight = \"normal\")\n      ),\n    locations = cells_body(rows = Oct_2021 > Oct_2022)\n  ) %>%\n  tab_style(\n    style = list(\n      cell_fill(color = \"#F58BA1\")\n    ),\n    locations = cells_body(\n      columns = c(Ref_Sep_2022),\n      rows = Ref_Sep_2022 < 0)\n    ) %>%\n  tab_style(\n    style = list(\n      cell_fill(color = \"#F58BA1\")\n    ),\n    locations = cells_body(\n      columns = c(Ref_Oct_2021),\n      rows = Ref_Oct_2021 < 0)\n    ) %>%\n    gt_highlight_cols(\n    Type_of_Construction,\n    fill = \"#F2EEE9\",\n    font_color = \"black\",\n    font_weight = \"normal\"\n  ) %>%\n  gt_highlight_rows(\n    rows = c(1,2,3),\n    fill = \"#FBFAF9\",\n    font_color = \"black\",\n    bold_target_only = FALSE,\n    target_col = Type_of_Construction\n  ) %>%\n  gt_add_divider(\n    columns = \"Type_of_Construction\", \n    style = \"solid\", \n    weight = px(4)) %>%\n  gt_add_divider(\n    columns = \"Oct_2021\", \n    style = \"solid\", \n    weight = px(2)) %>%\n  gt_add_divider(\n    columns = \"Ref_Oct_2021\", \n    style = \"solid\", \n    weight = px(4)) %>%\n  fmt_currency(\n    columns = c(Oct_2022, Oct_2021),\n    currency = \"USD\",\n    decimals = 0\n  ) %>%\n  fmt(\n    columns = c(Ref_Sep_2022, Ref_Oct_2021),\n    fns = function(x) {\n      paste0(x * 1, \"%\")\n    }\n  ) %>%\n  gt_plt_sparkline(\n    trend,\n    type = \"ref_last\",\n    fig_dim = c(8, 40),\n    palette = c(\"black\", \"black\", \"#D9043D\", \"#0669BF\", \"#3E606F\"),\n    same_limit = FALSE,\n    label = TRUE\n  ) %>%\n  gt::cols_align(\n    align = \"center\",\n    columns = c(\"Oct_2022\", \"Oct_2021\", \"Ref_Sep_2022\", \"Ref_Oct_2021\")\n  ) %>%\n  tab_spanner(\n    label = md(\"**Percentage Change**\"),\n    columns = Ref_Sep_2022:Ref_Oct_2021) %>%\n  tab_spanner(\n    label = md(\"**Comparison**\"),\n    columns = Oct_2022:Oct_2021) %>%\n  tab_footnote(\n    \"Percentage Change referenced with October 2022\",\n    locations = cells_column_labels(4)\n  ) %>%\n  tab_footnote(\n    \"Percentage Change referenced with October 2022\",\n    locations = cells_column_labels(5)\n  ) %>%\n  tab_footnote(\n    \"Trends from October 2021 to October 2022\",\n    locations = cells_column_labels(6)\n  ) %>%\n  tab_footnote(\n    \"Preliminary\",\n    locations = cells_column_labels(1)\n  ) %>%\n  tab_footnote(\n    \"Revised\",\n    locations = cells_column_labels(2)\n  ) %>%\n  tab_footnote(\n    \"Annual rate; Data adjusted for seasonality but not price changes.\",\n    locations = cells_column_labels(0)\n  ) %>%\n  tab_source_note(md(\"**Table**: Jisell Howe, CDT | **Data**: U.S Census Bureau, Construction Spending, December 1st, 2022 <br> **Additional Information**: www.census.gov/construction/c30/meth.html\")) %>%\n  cols_width(\n    #Type_of_Construction ~ px(100),\n    Oct_2022 ~ px(100),\n    Oct_2021 ~ px(100),\n    Ref_Sep_2022 ~ px(30),\n    Ref_Oct_2021 ~ px(30),\n    trend ~ px(40)\n  )\n\nHere is the final table result for the Posit Table Contest:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, the GitHub repository includes the static HTML file for viewing the entire entry all at once. It includes the tables created with {R} and a series of line plots produced in {Python}."
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#line-plots-with-python",
    "href": "posts/posit-2022-table-contest-entry/index.html#line-plots-with-python",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Line Plots with Python",
    "text": "Line Plots with Python\nBeyond the three tables I created with {R}, I wanted to take it a bit further with visualizing the time series data, this time with {Python} as I am a little more familiar with it.\nIn much the same way, I brought in the three data sets again, cleaned them up, re-ordered them, combined them, and created a master data frame that was segmented per overall category. The same considerations with the data sources apply here too. Because these are actual links to data as opposed to static, the tables will update on a regular basis and we need to be mindful of what those changes mean in updating any “static” elements.\nFor these line plots, I used {pandas} to bring in the data and {matplotlib} for the display.\n\n\n# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pl\nimport matplotlib.gridspec as gridspec\n\n\n# read data June 2022 - Oct 2022\ndf = pd.read_excel('https://www.census.gov/construction/c30/xls/release.xlsx', skiprows=4)\n\n#df = pd.read_excel('release.xlsx', skiprows=4) #alternatively, please use the provided spreadsheets on GitHub in a static way\n\ndf_1 = df.dropna()\ndf_1 = df_1.iloc[ : , :7]\nnewcolumns1 = [\"Type_of_Construction\",\"Oct_2022\",\"Sep_2022\", \"Aug_2022\", \"Jul_2022\", \"Jun_2022\", \"Oct_2021\"]\ndf_1.columns = newcolumns1\n\nx1 = df_1.columns[1:]\nx1 = list(df_1.columns)[1:]\nx1.reverse()\n\n# read data Jan 2022 - May 2022\ndf = pd.read_excel('https://www.census.gov/construction/c30/xls/pr202205.xlsx', skiprows=4)\n#df = pd.read_excel('pr202205.xlsx', skiprows=4) #alternatively, please use the provided spreadsheets on GitHub in a static way\n\n\ndf_2 = df.dropna()\ndf_2 = df_2.iloc[ : , :6]\nnewcolumns2 = [\"Type_of_Construction\", \"May_2022\",\"Apr_2022\", \"Mar_2022\", \"Feb_2022\", \"Jan_2022\"]\ndf_2.columns = newcolumns2\n\nx2 = df_2.columns[1:]\nx2 = list(df_2.columns)[1:]\nx2.reverse()\n\n# read data Oct 2021 - Dec 2021\ndf = pd.read_excel('https://www.census.gov/construction/c30/xls/pr202201.xlsx', skiprows=4)\n#df = pd.read_excel('pr202201.xlsx', skiprows=4) #alternatively, please use the provided spreadsheets on GitHub in a static way\n\ndf_3 = df.dropna()\ndf_3 = df_3.iloc[ : , :5]\nnewcolumns3 = [\"Type_of_Construction\", \"Jan_2022_\", \"Dec_2021\", \"Nov_2021\", \"Oct_2021_\"]\ndf_3.columns = newcolumns3\n\nx3 = df_3.columns[1:]\nx3 = list(df_3.columns)[1:]\nx3.reverse()\n\n\n\n# combine data sets\n\ndf_m = pd.merge(df_1, df_2, left_index=True, right_index=True, suffixes=('_df1', '_df2'))\n\ndf_c = pd.merge(df_m, df_3, left_index=True, right_index=True, suffixes=('_dfm', '_df3'))\n\n\ndf_master = df_c[[\"Type_of_Construction\", \"Oct_2021\", \"Nov_2021\", \"Dec_2021\", \"Jan_2022\", \"Feb_2022\", \"Mar_2022\", \"Apr_2022\", \"May_2022\", \"Jun_2022\", \"Jul_2022\", \"Aug_2022\", \"Sep_2022\", \"Oct_2022\"]]\n\n\nx = df_master.columns[1:]\nx = list(df_master.columns)[1:]\n\n\n#segment data based on construction category. The data sets feature various categories together.\n\ndf_all = df_master.head(n=19)\ndf_private = df_master[20:35]\ndf_public = df_master[36:50]\n\nOne challenge I had also carried through from the sparkline challenge I had earlier. I needed to find a way to take several columns of time series data and make them go into lists for any respective construction category and in the correct time series order. Here is how I ended up doing that.\n\ny5 = df_all[df_all[\"Type_of_Construction\"].str.contains(\"Office\")]\ny5 = list(y5.loc[7])\ny5 = list(y5[1:])\n\nWhat this does is locate a row that has “Office” as the construction type within the “All Construction” data frame. It then takes in multiple columns worth of data and excludes the first value in the column “Type of Construction” to come up with a list of only costs over time for “Office.”"
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#conditional-formatting",
    "href": "posts/posit-2022-table-contest-entry/index.html#conditional-formatting",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Conditional Formatting",
    "text": "Conditional Formatting\nAfter I created several lists of data, I moved on to visualizing the lines. I created some functions that would automatically format the lines based on whether the spending was declining or increasing. I also included a secondary version of the functions where I could differentiate a few lines further if I wanted. I did this for highlighting certain lines beyond simply negative or positive. This function went into the code as a variable instead of specifying an actual color.\nHere is a sample of what that looks like:\n\n\n\n# Plot the line charts - All, Private, Public \n\n\n# set the conditional color\nc_positive = \"#D9E8F5\"\nc_negative = \"#F58BA1\"\nc_positive_s = \"#0B508C\"\nc_negative_s = \"#A61C41\"\n\ndef colorizer(y):\n  if (y)[0] > (y)[-1]:\n    return c_negative\n  else:\n    return c_positive\n  \n\n\ndef colorizer_s(y):\n  if (y)[0] > (y)[-1]:\n    return c_negative_s\n  else:\n    return c_positive_s\n\n# set the conditional marker\nm_positive = \"^\"\nm_negative = \"v\"\nm_positive_s = \"^\"\nm_negative_s = \"v\"\n\ndef marker(y):\n  if (y)[0] > (y)[-1]:\n    return m_negative\n  else:\n    return m_positive\n  \ndef marker_s(y):\n  if (y)[0] > (y)[-1]:\n    return m_negative_s\n  else:\n    return m_positive_s\n\n  \n# additional styling options\nlw=2\nlw2=2\n\nax1.plot(x, y5, marker=(marker(y5)), lw=lw, color=(colorizer(y5)))\nFrom there, I adjusted the x-ticks, y-ticks, title, and created annotations for notable points on the charts. I had long debated over the layout of the plots because I wanted the changes to be more apparent, but I also didn’t want them to look misleading in any way with different y-axis scales. I went with a larger chart showing All Construction and then a split the space next to it for Private and Public Sector. I also heavily annotated critical points, so there was no confusion on the insights I wanted to show. I purposely left out “overall summary categories” because all the sub-categories were already shown on the charts.\nHere is the final result of the line plots. These annotations would be examples of items that would likely need to be updated if you refresh the code and data beyond the date of this publishing.\n\nI have had various experiences in data visualization in the last couple years, ranging from Tableau to Power BI to {Python} and now {R}. {R} is the newest tool in my toolbox, so I saw this as the perfect opportunity to take something from start to finish with raw data. This was certainly a stretch assignment in both {R} and {Python} for me. I learned a lot from participating in this contest, and I hope to participate in more projects like this in the future. Thanks Posit!"
  },
  {
    "objectID": "posts/posit-2022-table-contest-entry/index.html#bonus-presentation-of-tables-and-visuals",
    "href": "posts/posit-2022-table-contest-entry/index.html#bonus-presentation-of-tables-and-visuals",
    "title": "Posit 2022 Table Contest Entry: Construction Spending",
    "section": "Bonus Presentation of Tables and Visuals",
    "text": "Bonus Presentation of Tables and Visuals\nHere is a poster style presentation of all the tables and visuals side by side just for fun. To be clear, this part goes beyond the actual Posit Table Contest entry in that this part didn’t involve code at all and is not the generated HTML file from the code. This last part was done for social media purposes only. Please check out the GitHub repository for all files related to this project. Thanks!"
  },
  {
    "objectID": "posts/sankey-charts-creating-a-fun-slide-in-tableau-public/index.html",
    "href": "posts/sankey-charts-creating-a-fun-slide-in-tableau-public/index.html",
    "title": "Sankey Charts - Creating a Fun Slide in Tableau Public",
    "section": "",
    "text": "A Sankey diagram is a data visualization technique that shows flow from one point to another. The flows help show connections between a beginning and a possible end.\nThis week’s MakeoverMonday challenge focused on the housing outcomes of clients of the Australian Specialist Homelessness Services. Clients could go from one housing situation to a multitude of other housing situations. Because of this, I decided to make use of a Sankey chart to show where the flows were going.\nThis became a bit of a challenge because the tutorials I saw on how to make one in Tableau seemed to reference features that Tableau Desktop would have but not Tableau Public.\nHowever, I got one to work and I will show you how I made it!\n\nCreating a Sankey Chart\nThe most crucial part in creating this diagram is dealing with the source data itself. Because the data needs to flow from one area to another, we need to tell Tableau to understand that within the same data set. For that reason, I dragged and dropped the Data sheet into the box below.\nI\nThe next step is to then drag and drop over that same sheet again to Union it with the first one. I then edited the connection to make sure it created a union. This creates a new column that will differentiate “Data” and “Data1.”\n\nFrom here, I went into the worksheet and started creating some calculated fields.\n\n\nCreating the Curve of the Sankey Chart\n\nThis first one will help build a flow pathway between the “two” data sources. This calculated field will separate the “two” by assigning them to a number. This says if it exists in the one data set, then it’ll be assigned a 1. If not, then it’ll be assigned a 49. In the end this field won’t be on any shelf for the data visualization, but it’ll help us build the curve with subsequent calculated fields.\n\nAfter I created this calculated field, I then right clicked to create a bin to essentially build a bridge between 1 and 49, the two data sets.\n\nWhat this will do is not only have 1 to 49 between the two data sets but it’ll also put numbers in between to help build that “bridge.”\n\nThis next part here might look like random things are being done, but actually we will be utilizing a mathematical function to build all the curved lines between the two data sets. This calculated field is part of that formula and will help space everything out, much like how I did for my jitter plot tutorial post.\n\nHere is the function we are trying to simulate. It’s called Sigmoid function, and as you’ll see in the next couple steps, it uses t from the calculated field that was just created. This will create the “S” shape between the two data sets.\n\n\n\nImage from Wikipedia\n\n\n\n\n\nImage from Wikipedia\n\n\nHere is how that function plays out in Tableau when we create yet another calculated field.\n\nNext, we need to rank each data set, so the Sigmoid function knows to flow from one point to another point.\n\n\nNow we can finally build the curve with all the different parts.\n\n\n\nRefining the Curve for the Sankey Chart\nDrag Padded to the details on the marks card and drag t to the columns shelf. Right click to Compute Using > Padded. Drag Curve to the rows shelf. As you can see, we’re finally getting somewhere!\n\nRight click on either t or Curve and go to Edit Table Calculation. This will open up a prompt where you can specify how the calculations are done. This first one shows that Rank1 will be computed using Specific Dimensions such as Client situation at first presentation, Client situation at end of support, and Padded.\n\nThe little drop down arrow next to Rank1 will then let you go to the next nested calculation to configure Rank2. Rank2 will be computing the same things as Rank1. And here is what the nested calculation t will need.\n\n\n\nFormatting\nNow for my favorite part, formatting things and making it all come to life.\n\nWe can right click and edit the axis on t and Curve to clean things up a little bit.\n\n\nNow, we want the data to show lines, not circles. To do that, we will need to change the marks to be lines and then drag the Padded bin to path. As you recall, that Padded pill had data points from 1 to 49 all spaced out. Now this creates that same bridge, but as lines.\n\nHere I created yet another calculated field because I want these lines to be thicker or thinner based on the number of clients that follow that path.\n\nHere’s how to put that calculated field to work.\n\nThen we can go to the change the size on the marks card.\n\n\n\nCreating the Pillar Columns\nThis next part involves additional worksheet tabs. I wanted to create a couple columns to go on each side of the Sankey diagram. One column will show the proportions of where people were at in their housing situations at first presentation and the next column will be where they are at after receiving support. These columns will act as “pillars” for the Sankey chart to flow between. I created this first column for “Before” and then duplicated the worksheet to slightly modify it for “After.”\n\n\n\n\nI then went back to the Sankey Chart worksheet and dragged Client situation at first presentation pill to the colors on the marks card. I’m not a huge fan of the colors because it just looks like a ton of colors at once and is a little hard on the eyes. I later changed these colors.\n\nIn the dashboard view, I decided to use floating objects to help position the columns on each side of the Sankey chart. Had I used tiled, it would have wanted to line up everything but might not have done exactly what I wanted.\n\n\n\nActions\nTo help deal with the color all over the place, I decided to utilize actions in the dashboard to help highlight only what I wanted when someone hovered over a section. In this case, if you hovered over a specific housing situation in the the “Before” column, it would then highlight all the paths that went elsewhere in the “After” column. This would show that people not necessarily kept the same housing situation after getting support.\n\n\n\n\nThe Final Data Visualization\nHere is what I came up with in the end. And here is the interactive data visualization. It was a bit of a challenge to make this chart do what I wanted from loosely following this tutorial. Ultimately, I would have loved if all the Sankey curves could have been gray and then I could highlight in color what I wanted when the Before and After columns were hovered over.\nI couldn’t quite figure that one out for this, and the highlighting actions tutorials I explored seemed to not follow the Sankey curve and created straight lines of highlighting, which is not what I wanted. If anyone has any tips on how to make better highlighting actions for a Sankey curve specifically, please do let me know!"
  },
  {
    "objectID": "posts/the-benefits-of-data-visualization-in-the-aec-industry-pt-1/index.html",
    "href": "posts/the-benefits-of-data-visualization-in-the-aec-industry-pt-1/index.html",
    "title": "The Benefits of Data Visualization in the AEC Industry - Pt. 1",
    "section": "",
    "text": "Data Visualization is one of my newest skills I am working on currently. Some of you may know that I recently completed a Coursera course called Data Visualization and Communication with Tableau.\nI did this because I found a spark of joy from mixing data analysis, creativity, and communication all into one.\nHow does it relate to what I’m currently doing now in my career? Good question.\nCAD management did not inherently start out as data management and data visualization for me, but it sort of evolved into it. Why? Because there are so many files that hold a lot of data within them. Historically, I have had to open them up side by side to visually compare. Doing things like that just doesn’t cut it anymore, especially when it comes to BIM content and data management in projects. I strongly believe that “data driven” and “data informed” decisions are more important than ever.\n\nData Visualization at Autodesk University\nIn November 2019, I took multiple classes on getting data out of files and into Microsoft Power BI dashboards. One of my favorite examples was this class that showed how to create a Power BI dashboard that displayed pictures, product information, and more from an Autodesk Fabrication CADmep database. Here is more information regarding how it works.\n Image retrieved from Darren Young’s website\nLive interactive dashboard by Tyler Phillips\nWith a little elbow grease, you could set up a data visualization that would provide insight all in one place. This is great for multiple audiences, especially those who do not have the design software installed but are interested from a management standpoint.\nThose of you who have attended Autodesk University know that the experience is basically like drinking from a fire hose with the plethora of information available. With that in mind, I took data classes like they were M&Ms.\nI used just about any free moment I could in Las Vegas to create my own Microsoft Power BI dashboard for analyzing Fabrication CADmep content before I left. I knew it might be a while before I could get back to it at the office (and I was right).\n\n\nMy own use of Microsoft Power BI\nI would be remiss if I didn’t mention my own uses of Microsoft Power BI dashboards. While the nature of my work is often non-public and/or confidential to be able to show specifics, I can say that one of my most prominent uses that I think anyone can relate to is CAD and BIM content standards with naming conventions.\nIf there is a lot of BIM content available and several attributes that were never standardized over time, then it creates a nightmare of similar yet different names. Think like Revit parameter names, naming of families, etc.Non-standardized naming conventions can make things hard to find and use in projects. And when things are hard to find, it creates potential for non-value added duplicates, rework, and otherwise waste.\n\n\nData Visualization Example: Microsoft Power BI Tree Map\nThat’s where a Microsoft Power BI tree map chart comes to the rescue. Once I extracted and prepared all the data from all the Revit families, I was able to load it into Power BI.\nHere is an example of what that could look like in a general visual sense. I broke it down into three main sections:\n\n\n\nExample tree map chart in Microsoft Power BI prior to applying best practices with color. Data has been anonymized.\n\n\n\nLarger rectangles to show that more of certain property names existed in most/all the data. Standardization likely already exists here.\nSlightly smaller rectangles show that there are duplicated/similar names being used. For example, “Item #” and “Item Number” might be used interchangeably in the entire BIM portfolio and could benefit from normalizing the data to just show one of those.\nThese are considered one offs for the most part. They are usually names of things that likely only apply to a few Revit families and not much else. For example, a radiant manifold might have some other named attributes compared to a ball valve because they are totally different things. There might be some exceptions to this, of course, but for the most part I am most concerned with the second section when it comes to standards in BIM content.\n\nFrom here I can do a few pie charts to decide on what to do in specific instances if the goal is simply to standardize. The pie chart would show which name occurs most often as part of a proportion.\n\n\nThe Takeaway\nSure, I could have tallied up numbers in a spreadsheet, but the tree map provided additional insight. The additional insight came from the visualization itself because it told a story. In this way I was able to use Microsoft Power BI to help make informed decisions on standardization.\nKeep in mind that this application is for exploration and maintenance of existing content. It could even apply if you acquire content over time across collaborative teams. If you have perfectly standardized BIM content at all times forevermore, then just scroll on :).\n\nSources:\nPhillips, T. (2019). Navigating Fabrication BIM Content with PowerBi. [online] Available at: https://www.linkedin.com/pulse/navigating-fabrication-bim-content-powerbi-tyler-phillips/ [Accessed 27 Dec. 2019].\n\nYoung, D. (2019). Microsoft PowerBI w/Autodesk Fabrication – BIM there. Done that.. [online] BIM there. Done that. Available at: http://www.darrenjyoung.com/2019/03/16/microsoft-powerbi-w-autodesk-fabrication/ [Accessed 27 Dec. 2019]."
  },
  {
    "objectID": "posts/the-benefits-of-data-visualization-in-the-aec-industry-pt-2/index.html",
    "href": "posts/the-benefits-of-data-visualization-in-the-aec-industry-pt-2/index.html",
    "title": "The Benefits of Data Visualization in the AEC Industry - Pt. 2",
    "section": "",
    "text": "Data visualization can do more than just tell us what is in a set of files for BIM management, as we saw in Part 1 of this blog post.\nIt can tell you trends, insights, health stats, and more. This happens all the way from the manufacturing shop floor to construction on site to facility management. Just a quick Google search will tell lots of stories about data visualization in the industry: here, here, oh, and here.\n\nThe Bigger Picture\nBut what is the underlying message to all of this? Data can tell a story, and it’s up to us to tell the story the best we can. Otherwise, it’s just a bunch of…well…data.\nHow do we do that?\nWe need to think of the audience. If we don’t consider who is viewing the data, the message will be lost. How can you convey the message to someone who isn’t familiar with the data? Especially if you are trying to provide insight for the use of BIM content? It turns out that there are best practices for doing exactly that.\nMany examples I have seen in Microsoft Power BI show a lot of information and colors all at once. That might be great for your own use behind the scenes.\nI now invite you to consider some of the lessons I am learning through multiple avenues:\n\nCoursera course on Data Visualization & Communication with Tableau\nBest Practices training on LinkedIn Learning\nMakeoverMonday community\n\n\n\nColor Matters\nHere is an example that shows estimated Christmas spending for 2019 in the UK as well as Europe. The example is obviously totally unrelated, but that is my goal for anything I do: to draw parallels from different domains and sources to provide new insight.\n\n\n\nData Visualization showing 2019 Estimated Christmas Spending in UK and EuropeData source (Deloitte United Kingdom, 2019)\n\n\nAt first glance, you just see everything colored. What do the colors mean? Categories where spending occurred? Okay. But beyond that, what is a key message for why this data visualization was created?\n*crickets*\nThis is where storytelling and color can come into play.\n\n\nStorytelling with Data\nWhat is the data telling you? Is there anything significantly different or interesting?\nAlmost every category has about the same amount of spending in both Europe and UK, except for one: gifts.\nWhy is there more estimated spending in UK in this category? This could warrant further research. Before you know it, there could be quite the insight behind why this has occurred. This is how you tell your data story.\nThe thing is, it’s difficult to see that when everything has its own color with no discernible purpose. For this reason, I have learned to be careful with color and draw attention to items with color.\nIf you must use more than one color to illustrate a specific data story, be mindful. The colors pink and red are very close together and therefore will hide any dramatic significance.\nAlso, there could be viewers who experience some form of being colorblind. Color choices like green and red might need to be reconsidered, depending on context, of course (Shaffer, 2016). More information regarding designing colorblind-friendly data visualizations can be found here.\n\n\nLack of Color Also Matters\nThat the lack of color also matters in a data visualization. Gray cuts out the “noise” and guides the viewer to where you actually want them to go.\nFrom here, additional formatting embellishments could call out specific numbers and other annotations to further bring the point home.\nHere is an example of what could make the first chart look a bit better.\n\n\n\nUsing color to draw the viewer’s attentionData source (Deloitte United Kingdom, 2019)\n\n\nFor an industry specific example, we could revisit the treemap example I wrote about in Pt. 1 of this blog post series. Rather than having several colors with explanations about everything, we could use a diverging color scheme to point to the “medium sized” rectangles as a place to look in our standardization efforts.\n\nBefore - Treemap with No Color Consideration\n\n\n\nAfter - Treemap with Mindful use of Color\n\n\n\nEvaluating BIM content quality in terms of consistency across a library. Note: No specific BIM library is to be assumed here in this example as the data is presented for illustration purposes only.\n\n\nI wrote more about this treemap and more in a more recent post here.\n\n\n\nMakeoverMonday\nAre there other ways to improve upon data visualizations? Absolutely! This is why I enjoy participating in what is called MakeoverMonday. This is a data community event where people are provided with publicly accessible data to create data visualizations. Participants can share their results on Twitter as well as Tableau Public. The one illustrated above came from just one of the data sets provided via the MakeoverMonday folks.\n\nHere’s the fun part. MakeoverMonday provides a means to practice your skills in a fun and creative way. A book was even written about it, so you can get involved in a great community.\nBy being more mindful of your data and the communication surrounding it, I believe we can influence peers and decision makers in the AEC industry. Executing BIM projects provides us just one of multiple opportunities to utilize data for better insights. Whether you’re a CAD/BIM manager making things happen in the name of technology or a full blown data scientist or business analyst, we know it’s more than just a “pretty dashboard.” Let’s prove it.\n\nSources:\nDeloitte United Kingdom. (2019). Deloitte Christmas Survey 2019. [online] Available at: https://www2.deloitte.com/uk/en/pages/consumer-business/articles/deloitte-christmas-survey-2019.html [Accessed 28 Dec. 2019].\n\nShaffer, J. (2016). 5 tips on designing colorblind-friendly visualizations. [online] Tableau Software. Available at: https://www.tableau.com/about/blog/2016/4/examining-data-viz-rules-dont-use-red-green-together-53463 [Accessed 28 Dec. 2019]."
  },
  {
    "objectID": "posts/the-quest-for-the-perfect-bim-project/index.html",
    "href": "posts/the-quest-for-the-perfect-bim-project/index.html",
    "title": "The Quest for the Perfect BIM Project",
    "section": "",
    "text": "In November 2017, I was invited to take part in a panel at Autodesk University on standardizing BIM content. There were “stakeholders” represented, such as an architect, engineer, and contractor.\nWe discussed manufacturer BIM content, LOD in Autodesk Revit, and uses of BIM content throughout the project life cycle.\nI learned that there were several different needs and ways in which each “stakeholder” used Revit content. These insights gave me the impression that the quest for the “perfect BIM project” apparently holds multiple definitions.\nSome people may find that manufacturer specific Revit content is too much for their needs in a certain stage of the project, so they either scrub models or create them from scratch. It made me wonder, is the amount of detail just one view of being perfect?\nOthers may find that some manufacturer Revit content risks being way too detailed and complex down to every little modeled nut and bolt on a piece of equipment. A much simpler model that gets the job done is preferred. Is this another view of being perfect?\nSince Revit is just one vehicle for BIM, there is something else to consider, and that lies with Autodesk Fabrication CADmep content and beyond.\nIn a general sense, Fabrication content really places its importance on accurate dimensions, so that models can later be visualized and help prevent issues in the field. That’s all well and great. However, there is something I would like to bring more attention to in the industry.\n\nVirtual World vs. Real World Construction\nTo preface this next part, I want to reference to an article I read on LinkedIn, very appropriately called “Level of Accuracy: The Virtual World vs. Reality and Cost to Our Business.”\nThis excellent article by David Francis talks about what actually happens on the job site compared to what is created digitally.\nWorking for a design department within a manufacturing company has offered me a wonderful intersection and perspective to explore. I not only support the creation of BIM content for designer and customer use, but I also understand the data that goes into it.\nI can say that while excellence is the goal, I can also say that manufacturer product tolerances do indeed exist. If they didn’t, so many more products would be rejected on the shop floor, or there could be even more variance.\n\n\nLosing the Forest for the Trees\nFor someone who does not install or build on a daily basis, it can be so easy to get lost in the numbers. I’m talking about very precise dimensions in software that a standard tape measure can’t even compete with, simply because software can be so precise these days. As the article I referenced on LinkedIn mentions, things like expansion/contraction, the other properties of pipe, and the skill level of the person installing can also affect things in real life.\nHow can you account for those things in software, especially in the case of an installer’s skill level making the difference? Hmm.\nConstruction technology tools can be amazingly accurate, but they cannot beat actual on site experience, no matter how much we all strive for quality. It’s on site where I believe you can gain experience in what matters most in design. Now join me as we take a trip in a time machine.\n\n\nMy Job Site Experience\n\nAbout eight years ago in my early career, I went on more job sites and could see immediately that the radiant heated flooring tubing design I may have drawn up beautifully in software looked slightly different in real life on the job site. A small obstacle that wasn’t on the plans perhaps caused a slight detour in installation. The spacing of the radiant loops for a given size of tubing didn’t result in a perfect bend, even though I made it do it perfectly in design software. Regardless, the design calcs were still spot on and the product as designed served its purpose.\n\n\nHabitat for Humanity\nOver ten years ago, I also built scaffolding up multiple stories (yikes), installed windows, and put up drywall as a volunteer for Habitat for Humanity.\n\n\n\nHabitat for Humanity: Women Build 2009\n\n\n\n\n\nWomen Build 2010 with Builders Association of the Twin Cities (BATC) - now called Housing First Minnesota\n\n\n\n\n\nHabitat for Humanity: Women in Construction 2011 with APi Construction Company\n\n\nI’m thankful that I was able to get opportunities over the years to participate in these valuable experiences. They showed me that real life happens on job sites, no matter what is done in a perfect software design. Not only that, I really enjoyed being out in the open air on a wonderful summer day for a change.\n\n\nBIM Standards and Practices\nSo, this bring me back to my original points. It seems that in the quest for the perfect BIM project, there could be multiple interpretations and ways of doing things. Industry wide BIM standards can be a challenge to implement; I’ve seen valiant efforts there. Software changes over time to allow new features, so that also changes the game a bit as well.\nAdditionally, one person’s “easy button” can be another person’s headache. One person’s perfect Revit family can be another person’s head scratcher. I think that the potential culture of perfection might be clouding what I think brought many of us to the industry in the first place.\n\n\nTrade School Experience\nWhen I first attended college at Dunwoody College of Technology for my Architecture degree, I felt like I was going to Harry Potter’s School of Witchcraft and Wizardry. Hear me out.\nI felt like I was learning some amazing hands-on skills. I built model houses from foam and wood, did surveying in the school hallways with theodolites, and took pictures on job sites for site analysis. When I wasn't doing those things, I was hand drafting, doing lettering assignments, drawing in AutoCAD, and modeling in Revit. It was like I was learning the \"magic\" of the trades. And, I was following in my dad's footsteps. Bonus.\nBy the way, I'm stuck with lettering now for the rest of my life. It's either lettering or cursive. There is no in between ;).\nToday's education in this industry might put a lot of focus on the latest and greatest technology. But I do think that regardless of the education, we need to remember something very important:\nNo matter how perfectly we design today, real life will always happen on the shop floor and the job site.\nRealizing this will help us form collaboration strategies for what is actually critical and build our technology around that.\n\n\nTwitter Poll & Responses\nI put an informal pollon Twitter the other day. The idea behind it was to see how many people had first hand experience participating in the built environment. It's important to note that the results are from the people who saw the poll and chose to respond; it isn't a statement about everybody.\n\n\n\n79.7% have been out in the field many times, while 20.3% have been out in the field less than 10 times.\n\n\nIf I were to answer the poll myself, I would probably be at about 10+ formal events. These include job site visits for site analysis, volunteering days at Habitat for Humanity, visiting multiple manufacturing and fabrication shop floors, and physical product installation training at work. It doesn't include the countless bird houses and tree house projects I worked on with scrap wood as a kid. It also doesn't include the regular collaboration with folks in the manufacturing office that I currently do.\n\n\nPoll Comments\nAdditionally, I had supporting comments, conversations, and clarifications that I think really bring up some excellent points. Doing is different from seeing, and being there is different from not being there. Here is just a small selection of them and there are a lot more that came in here on the original post. Thank you for the comments!\n\n\n\n\n\n\n\n\nThis poll shows that experience on job sites is really important to have for having a career in this industry. The bright and shiny software tools are there to help, but they are not to replace real world experience.\nAs you can see, many cite that \"the experience is invaluable\", that it made them \"rethink how detailing is done\" and that it made them better at what they do now. That's not to say that communication shouldn't happen both ways. Everyone needs to be on same page, regardless if in the field or office.\nI think this last comment really resonates with me simply because I really enjoyed my own experiences being on site.\n\n\"Regardless of anything successful I might have done recently, they are still the best years of my professional life.\"\n– Chiara Rizzarda\n\n\nI felt that everything I did hands-on had so much meaning. I can now drive by the houses I worked on years ago and say \"I installed the windows there\" or \"I put up sheetrock in their basement.\" It's a solid case of accomplishment.\n\nIt's not that what I do now does not have meaning or that I do not love it, but there is something special to be said about seeing the actual result of something in real life. I had a wonderful time going down memory lane and reflecting on old photos for this post.\n\n\nCall to Action: Learn and Communicate\nIf you have not been on a job site as often as others, don't worry, it's not a competition. It's a point of self awareness to maybe bring about a valuable opportunity to get more involved. Tag along to an install at work, see what it's really like to build something, and address disconnects between field and office. You may be very surprised at the results.\nEven for those who have been on site many times, different products and trades offer different experiences as well. Nobody should be immune to learning something new. Just the effort of getting out of your comfort zone to see another perspective will do wonders for communication. We can then get better outcomes together, whether it is building BIM content, coordinating construction meetings, or getting steel toe boots muddy on site. I'm always willing to learn to improve my insights as well, of course.\nIn closing, strong communication across the trades and between office and field is what will ultimately make the buildings stand up, not the allure of digital perfection.\n\nDisclaimer: Views expressed here are my own and are not endorsed or otherwise affiliated with my employer.\n\nSources:\n\nEngel, M. [mengelmn]. (2020, January 4th). It would be of interest to [k]now the breakout. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nFrancis, D. (2015, December 22). Level of Accuracy: The Virtual World vs. Reality and Cost to our Business. Retrieved January 7, 2020, from https://www.linkedin.com/pulse/level-accuracy-virtual-world-vs-reality-cost-our-business-francis/.\n\nMaller, A. [Twiceroadsfool]. (2020, January 4th). For sure. I think it should be a mandatory part of architectural licensing. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nMackey, B. [TheRevitGeek]. (2020, January 4th). [A]greed most designers say they learn a ton. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nMackey, B. [TheRevitGeek]. (2020, January 4th). While at the woodworking shop I used to do site installations. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nIrwin, P. [BIMchiq]. (2020, January 4th). Spent a whole year onsite doing full time CA. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nRaynor, D. [Revit_Dr_MEP]. (2020, January 4th). I worked before college as an HVAC technician. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nRevolution Design [Rev_Design]. (2020, January 5th). Intentionally had summer jobs framing houses. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nRizzarda, C. [CrShelidon]. (2020, January 4th). I started my career as a site surveyor. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384\n\nThomas, J. A. [jadamthomas]. (2020, January 5th). Started in family construction business with residential. [Tweet] Retrieved from https://twitter.com/ji5ell/status/1213491064214032384"
  },
  {
    "objectID": "posts/waffles-anyone-an-alternative-to-pie-charts/index.html",
    "href": "posts/waffles-anyone-an-alternative-to-pie-charts/index.html",
    "title": "Waffles Anyone? An Alternative to Pie Charts",
    "section": "",
    "text": "As popular as pie charts may be in many different settings, they sure seem to have a bad reputation in the data visualization world.\nWhy is that?\nConsider this for a moment. A pie chart with many different pieces (and colors) that may or may not be labeled. The viewer then has to study the proportions in relation to the others to understand it. The reality is that we aren’t that great about reading and comparing angles to each other, and the more proportions there are to look at, the worse it gets. A 3D pie chart can make the problem even worse because the “perspective” view can deceive the reader into thinking a proportion is larger than it is.\nFor example, the red piece in the pie chart below seems to be bigger, but the blue piece sure seems to look pretty close in size as well. Additionally, is the blue piece larger than pink and green combined?\n\nSee what happened there? You’re thinking about it too much instead of gathering the clear actionable insight.\nWhile there might be some instances where pie charts might be useful, I’m here today to show you a (fun) and easy to read alternative.\n\nIntroducing the Waffle Chart\nWaffle charts are pretty much exactly what they sound like. They look a lot like waffles. These waffle charts, however, feature a neat way to show proportions of data. Let me show you how.\n\n\nContext Behind the Data Set\nThe data set for #MakeoverMonday Week 6 looked at the percentage of someone’s life that involved the U.S. being at war. This percentage varied per birth year because of different conflicts over time. The point was that nearly a quarter of Americans have never experienced the U.S. in a time of peace. Here is the original data visualization that was provided with the data set.\n\n\n\nData Source: Washington Post\n\n\nAs you can see, that is a ton of pie charts! Many of them look very similar at first glance, but they actually represent different percentages over time. The angles aren’t very distinctly different. For this reason, I wanted to explore a different way to show proportion over time. This is where the waffle chart comes into play.\n\n\nWaffle Chart Template\nFor waffle charts to work, there is some Tableau magic that needs to take place. I’ve seen a [few different ways](\nWaffle Chart In Tableau\n) to accomplish what I ultimately wanted to do, and what I will show here seemed to be the easiest route for me.\nThe first step is to actually create a “template” to use with your other data source(s). The waffle creates the framework and the data will show through it in the form of colored boxes (at least in this case).\nIn a spreadsheet, I laid out a template with a Row, Column, and Percentage columns. I only show part of it here in the screenshot because the information repeats itself.\n\n\nIn the Row column, you’ll want to count up to 10 and have ten repeated instances of each number.\nIn the Column column, you’ll want to count up to 10 and repeat every ten lines.\nIn the Percentage column, you’ll want to simply ascend in percentages from 1 to 100%\n\nI saved this spreadsheet as the Waffle Chart Template and then opened up Tableau Public.\nHere is what the waffle chart setup looks like initially\n\nYou can flip the 1% through the 100% around in different ways. Color will later replace the numbers. So in this example, 18% will fill up most of the bottom two rows in color.\n\n\nColoring the Boxes\nThe next step is to create a couple calculated fields.\nThe first one will involve our war source data itself. As you can see in the screenshot in the top left, the data sources you click on will show the available pills, so we need to be mindful of that when we make calculated fields that go with specific data.\n\nWhat this calculated field will do is create a ratio of the amount of time at war for any given birth year. It seems that the data source formatting can make the difference of what exactly gets placed in the calculated field, but this is what I was able to make work in this particular case.\nWith Waffle Chart Template selected in the top left, I created this calculated field. As you can see, the fields in the calculated field (in orange) look a lot like a typical Excel spreadsheet where you are referencing items from different worksheets in a workbook. It will show <Source.Field> in terms of formatting.\n\nWhat this second calculated field will do is color in the waffle chart boxes based on the condition that the life at war scenario is greater than or equal to percentage numbers we came up with in the waffle chart template.\nIf that condition is “true,” then the boxes will be colored one color. If it’s “false,” then the rest of the boxes will be colored a different color.\nHere’s how that plays out.\n\nWe will need to filter by a single year on whatever chart we show. Otherwise, the colored boxes won’t know what to do with multiple different ones selected.\n\n\nFormatting the Waffle Chart\nIn this next step, I typed in a new field in the column shelf called AVG(1). What this will do is help us make all the boxes become more uniformly distributed and lined up.\n\nI then right clicked on the axis below to edit it and make it fixed. What that does is remove the 0.0 to 1.0 points and scoot all the boxes together.\n\nFrom here, I am able to move all the boxes together uniformly.\n\n\n\nAnnotations\nI then added a mark label here to show the percentage filled for this particular waffle chart.\n\nAs you may recall from my previous posts, the default annotations and tool tips tend to show all the fields and I like to reformat them to read differently. In this case I just cut it down to just the percentage itself.\n\nThe original data source didn’t display the percentage with a percentage sign, so I fixed that with the number format options.\n\n\n\nColor Choices\nI also looked at coloring this waffle chart carefully. Gray would fade away while the red would highlight the percentage filled in the waffle chart.\n\nI initially went with red for an emotional impact regarding war. Later, I muted it a bit to be a dark pink instead. Too much red can be a strain on the eyes. My goal was to put a bunch of these charts side by side on the dashboard, so that helped influence my decision.\nNow that the waffle chart looks like what I intended, what I did next was copy and paste the worksheets. I then filtered for different birth years. For my purposes, I only did this for the years 1905, 1915, 1925, 1935, 1945, 1955, 1965, 1975, 1985, 1995, 2005, and 2015. It was a personal decision to try to show the same kind of idea but in a much more condensed way.\n\n\nBonus Data Viz: Timeline\nAfter I completed all my waffle charts, I wanted to show a little more context on the kind of wars and conflicts that were going on throughout time. I created a little timeline to place on my dashboard.\nTo make this, I quickly put together a new data source from scratch to connect to Tableau Public. This new data source simply listed the name of the war/conflict and the time in years.\nThen, I created a calculated field and typed this in. It will later help create 0 lines for a timeline.\n\nI placed the War pill and the Placeholder calculated field on the rows shelf. I also placed the Year Began and the Year Ended pills on the columns shelf.\n\nFrom here, I made the chart show dual axis and synchronized the two together.\n\nI then edited the axis to evenly space the years across the bottom.\n\nHere’s how that turned out after some additional formatting with the colors and lines.\n\n\n\nFinal Data Visualization\nHere is how the final data visualization with all the waffle charts and timeline combined turned out! Here is the link to the interactive viz. It looks best on desktop or tablet.\nAs with any of these data visualizations I show here, I make it a point to learn a lot along the way, so I can share what I have learned. This one took quite a bit of time to troubleshoot, but I think it paid off. What do you think?"
  },
  {
    "objectID": "posts/welcome-to-the-races-animate-your-line-graphs/index.html",
    "href": "posts/welcome-to-the-races-animate-your-line-graphs/index.html",
    "title": "Welcome to the Races - Animate Your Line Graphs",
    "section": "",
    "text": "This week’s data visualization exercise with MakeoverMonday is about comparing data over time. In this case, the exercise involved looking at an existing data visualization of rising and falling gas prices for both petrol and diesel fuel and suggesting improvements. I decided to animate this data among other things in my version of this data visualization.\nFor context, here is the original data visualization. One of the things I noticed right away is that the diesel and petrol fuel seem to follow the same kind of patterns throughout. The lines are so close together, and I’m thinking that maybe something else could be interesting here instead of simply comparing petrol and diesel fuel.\nHere’s how I brought the data to life.\n\nTutorial\nAfter connecting the data set to Tableau Public, I set out to show the petrol and diesel lines together right away. I did this by putting the diesel and petrol price fields on the rows shelf and the date on the columns shelf. I then made the charts “dual axis” by right clicking on one of the fields. Otherwise, the charts would have just been side by side with one line each.\n\nTo try to show a little more separation in those lines, I edited the axis on the left side of the chart to start it well after 0. Normally, I would not want to do such a thing because it can be really misleading. However, in this case, I’ll be annotating several points of both lines later on in this tutorial to make it clear what the numbers actually are.\n\n\nKeep in mind that we are not done yet with this dual axis chart. Dual axis charts are like two charts laid on top of each other. Since we changed the “start” of the chart on one, that means that we need to do the same on the other. Otherwise it would suggest that these lines are way further apart than they really are.\n\n\n\n\nWhat Looks Interesting?\nBecause I formatted my date field to go by months, this chart shows me the drops and rises over any given month for about 15 years. I find the circled areas interesting because of the huge drops and rises. It makes me wonder what would influence the changes and what they mean.\n\nIt is these drops and rises I hope to highlight as I set out to animate this line chart. Maybe specific relevant events happened at the same times as these huge drops and rises? That would help put more context to the data.\n\n\nLet’s Animate!\nTo initiate the creation of animations in Tableau, I dragged and dropped the date onto the Pages card. Noticed I formatted it to show as months. What that will do is offer an animation point at every month for every year. If I would have selected something like quarters, then it would have done it by quarters. One point to note is that if you have it saying “Month(Date)” on the columns shelf, then you’re going to want it to say “Month(Date)” on the Pages card then too. If you don’t match them, you’re going to have a bad time.\nWhile getting down to the exact day is nice, it seems to really bog down on the animation side because I can only pick three speeds to animate everything. Having it as months seemed to create a reasonable amount of time to watch an animation unfold over 15 years.\n\nHere’s an issue I came across when I went to animate the actual data points. If I do not specify certain settings, then the dots themselves will just dance along the page. Not very helpful.\n\nTo remedy the situation, I changed the shapes to be circles and then changed the settings to show history for all points with a trail. Changing the shapes to be circles seemed to help with showing the trails.\n\n\n\nLet’s Annotate!\nUsually when I annotate charts, I just right click where I want the annotation and then add it. In the case of animations though, this created a new challenge. Notice what creating an annotation at the point does.\n\nRight clicking on a point ends up being kind of difficult. April 2012 was a record high, but if I wasn’t careful where exactly I clicked, it would show March’s value annotated instead.\n\nAlso, if you restart the animation, the annotation stays there! That’s not good. :(\n\n\n\nAnnotating by Mark\nTurns out, there are multiple ways to annotate charts and one way that will work splendidly well in this case is to do it by “mark.” Notice that it was grayed out in the other screenshot. To be able to actually use Mark, I noticed that I need to be at the point of the animation I want to mark with an annotation. So, wherever the dots were trailing, that’s where I could add an annotation. Let me show you what I mean.\nFor illustration, I made the circles huge and backed up to April 2012 in my animation. THEN I was able to right click, annotate, and select mark. I did the same for any other point I found interesting.\n\nTo reiterate, you need to back up the animation to the exact point you want to add the annotation there. Otherwise the Marks option will be grayed out.\n\nHere’s a rough draft of all the points I am interested in noting on this chart. Notice that Tableau just puts generated field names in there. I feel that this is your chance to really communicate to a viewer. Don’t miss this opportunity to reformat these things to actually be readable!\n\nHere’s a quick example of formatting the tool tip and then formatting the annotations. Notice that I am using colors to call out Diesel and Petrol so then we can do away with needing a color legend.\n\nOne thing I want to note here is that sometimes annotations and tool tips can get goofy if you do not have the field to pick from. To make my tool tips as workable as I can right away, I drag and drop fields I need to “details” on the marks card. That way, they will show up as fields that can be in the annotations. If I don’t do that, then I risk not having it show dynamically changing information.\n\n\n\nFormatting the Final Viz\nHere are some more formatting choices I made by right clicking on the annotations and selecting “format.”\n\nAnd here is the final data visualization in action! Now the viewer can watch the changes over time and put some context to critical points using the information I provided along the side of the dashboard.\n\n\nLink to interactive data visualization"
  },
  {
    "objectID": "posts/wordpress-to-quarto-migration/index.html",
    "href": "posts/wordpress-to-quarto-migration/index.html",
    "title": "WordPress to Quarto Migration",
    "section": "",
    "text": "Three years ago, I made a plan to create a portfolio and blog website with a custom domain. It was late December of 2019 when jisellhowe.com was first launched on WordPress.\nI set up everything to make it happen as quick as possible (WordPress site, backup service, hosting service, plugins, etc.) While I felt accomplished at the time, I didn’t realize then that I would later despise all of it.\nFeeling overwhelmed by the WordPress editor and the guilt of maintaining (paying for) a site that I had largely avoided posting to after a while, I wondered if I would ever find a more cost effective and streamlined solution.\nIt turns out I have, and I came across it by chance from a LinkedIn post.\nAt first, I thought it was too good to be true. What is Quarto? How can it connect to GitHub? Does that really result in a website that is close to being free to maintain?\nHowever, upon investigating further, I was able to migrate from WordPress and a paid hosting plan to Quarto and GitHub Pages with a custom domain at a significantly lower cost.\nThere are some key steps I took to make this a reality.\n\nSet up a folder in a web browser called “Website Migration.” It really comes in handy in saving pertinent website links mentioned below.\nReview a webinar and corresponding presentation on how to get started with Quarto. Save these links to the folder created in Step 1.\nExploring a method of exporting static HTML from my WordPress site seemed to be a bit of a dead end. What I ended up doing to migrate all my existing posts was the following:\n\nUse the workflow of creating a Quarto blog post and then copy/paste all the text from the “preview” of my WordPress posts into RStudio. Doing so preserves all or most of the formatting. I did this for each post. Yes, it did take a little while.\nSave each picture with a numbered prefix in the same folder as the post. I put references to myself in my post text (i.e. “IMG1”), so I knew where to put my images.\nRemove links to other blog posts in the blog post content for now. Or, make note of them for later, so they can be linked properly once the new website is up. I did this because I didn’t know the exact website links I’d have once I got it all onto GitHub.\n\nSet up a repository on GitHub pages.\nConfigure GitHub and the Quarto YAML file to have files go to Docs.\nDownload/Configure GitHub Desktop, so it talks to the repository you created on GitHub Pages.\nOnce all posts are migrated, render with Quarto and publish it to the basic GitHub Pages site you have without the custom domain yet (USERNAME.github.io).\nLook at the DNS settings of the custom domain at the place where you bought it. For me, it was within a portal at BlueHost. Add DNS information (A, AAAA, CNAME, and later TXT for verification) to the DNS settings in the portal where you bought the domain (i.e. BlueHost). I did it in multiple ways, so my domain with “www” in front (subdomain) and without it in front (apex domain) would both work properly.\n\nHere is a simplified table of the kinds of DNS records you’ll need to have with respect to the GitHub documentation.\n\n\n\n\n\n\n\n\n\nDNS Record Type\nHost Record\nPoints To/Value\n\n\n\n\nCNAME\nwww\nUSERNAME.github.io\n\n\nA\n@\nSee GitHub for IP addresses to add\n\n\nAAAA\n@\nSee GitHub for values to add\n\n\nTXT\n_github-pages-challenge-USERNAME.example.com\nSee GitHub for Value\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake note of the DNS records referenced in the table and links above in case you need to add them again. In my case, I had to configure them twice because cancelling my hosting plan wiped out all the DNS settings for my domain, including the records I had personally added. I had to watch for the DNS settings to propagate again after doing so. Having these DNS settings exactly right will allow GitHub to recognize the custom domain properly.\n\n\n\nGo to the settings part of GitHub pages to add the custom domain. It should then generate the CNAME file on the GitHub side, so the two places talk to each other. Once GitHub considers it correctly configured, check the box to enforce HTTPS.\nVerify DNS settings globally in a variety of ways (A, AAAA, CNAME, and TXT). Once you see the correct values show up for what you set in your DNS settings earlier, the website is propagated globally. The TXT one especially showing up correctly allows for further verifying that custom domain on your GitHub account for security purposes.\nCancel any hosting plan as now the custom domain is verified and cleared to work with GitHub Pages. Keep the domain itself. Just cancel the hosting plan.\n\nYou are now seeing the results of my Quarto blog with a custom domain that was created with RStudio and GitHub Pages! Is it perfect? No, not quite. I am still learning how to incorporate various Markdown techniques and YAML configurations to make the most of my new blogging experience.\nThere are also some things that do not make this new site 100% equivalent to the old one:\n\nNo comments on individual posts or any sort of way to handle email from the website.\nThis is fine because sometimes I got random comments to “approve” before, and I wasn’t really interested in continuing that. I also wasn’t interested in any implications in “collecting user data” just to get a subscribe feature working. I am able to immediately link to all my social accounts in my blog.\n\n\nNo scrolling or fancy theme effects or plugins.\nThis is fine because I didn’t like paying for themes and plugins that would eventually break. I didn’t like placing “short codes” in the typical places where certain website elements would live in WordPress. It made it that much more of a challenge to troubleshoot things later. I still don’t know why some of my “published” blog posts disappeared from the front end of my old WordPress site.\nIn contrast, this Quarto blog has been so easy to troubleshoot any issue. Even the GitHub part wasn’t too bad once I figured out what needed to happen. I know the purpose of just about every file, each setting, and what it does. In addition to it being very easy for the whole to thing build and deploy, I am impressed with the aesthetic options. I paid for a WordPress plugin to make my blog posts show up in a certain layout, and Quarto offered a very similar aesthetic right off the bat for free.\n\n\nNo SEO tools.\nThis is fine because it’s a personal blog/portfolio. Even though I did see the value of the SEO plugin I had, I don’t necessarily care that much if my personal website/blog is not 100% optimized for search. That said, I can make use of the editing tools in my IA Writer Markdown editor. IA Writer offers “Syntax Highlighting” tools to help spot weak verbs, redundancies, and more. I know that’s not the same as SEO, but since I really only plan to post blog posts and links from a social platform, I’m not all that concerned. Plus, I could do with a little less judgment from those red, yellow, and green emojis in the Yoast SEO app anyway. If you use Yoast, you know exactly what I’m talking about.\n\n\nNo automatic website backup services.\nThis is fine because now my rendered files are on GitHub pages. I can also easily place any of my Quarto files there or even pursue something like Dropbox etc. Also, for some reason, I kept having failed backups with my WordPress site anyway. To pay a significant amount of money for something to fail and not really have time to fix it is the major reason why I scaled so far back on what I needed in a website.\nThroughout this migration process, I have learned so much on what the most critical parts are in creating a website. I can’t say I had the same experience with WordPress because there was so much automatically done on the back end that resulted in a ton of bells and whistles that I didn’t really need. WordPress certainly has its place, but it is too much for my needs at this time. In the end, this was an excellent project that resulted in a significantly cheaper alternative for managing a website. There is also a huge sense of accomplishment from actually figuring out how to do it."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "I deleted my browser bookmarks and emails. I do this instead.\n\n\n\nmarkdown\n\n\nobsidian\n\n\nnotetaking\n\n\nproductivity\n\n\nPKM\n\n\nsecondbrain\n\n\n\n\n\n\n\nJisell Howe\n\n\nDec 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Everyday Possibilities in Markdown\n\n\n\nmarkdown\n\n\nrecommendations\n\n\nproductivity\n\n\n\n\n\n\n\nJisell Howe\n\n\nDec 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosit 2022 Table Contest Entry: Construction Spending\n\n\n\nrstats\n\n\npython\n\n\ntable\n\n\nlinechart\n\n\nquarto\n\n\npersonalproject\n\n\n\n\n\n\n\n\nJisell Howe\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Verify your Quarto Blog on Mastodon\n\n\n\nhtml\n\n\nmastodon\n\n\nquarto\n\n\npersonalproject\n\n\n\n\n\n\n\nJisell Howe\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWordPress to Quarto Migration\n\n\n\nmarkdown\n\n\nquarto\n\n\npersonalproject\n\n\n\n\n\n\n\nJisell Howe\n\n\nNov 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Tips & Tricks - Python\n\n\n\npython\n\n\ncoding\n\n\ncourses\n\n\npersonalproject\n\n\n\n\n\n\n\nJisell Howe\n\n\nMar 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPicture Perfect Penguins - Dynamic Tableau Shapes\n\n\n\ntableau\n\n\nshapes\n\n\nscatterplot\n\n\ncustomfont\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\n\nJisell Howe\n\n\nJul 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBringing it into Perspective with Shapes\n\n\n\ntableau\n\n\nwafflechart\n\n\nshapes\n\n\nactions\n\n\nparameters\n\n\ncustomfont\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nJun 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s the Little Things - Small Multiples\n\n\n\ntableau\n\n\nsmallmultiples\n\n\ncolorblindfriendly\n\n\npiechart\n\n\ncustomfont\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nMay 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Project Spotlight - Constructing New York\n\n\n\ntableau\n\n\nareachart\n\n\nbarchart\n\n\nmap\n\n\nanimation\n\n\ncustomfont\n\n\nconstruction\n\n\nanalysis\n\n\npersonalproject\n\n\n\n\n\n\n\nJisell Howe\n\n\nMay 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the Races - Animate Your Line Graphs\n\n\n\ntableau\n\n\nanimation\n\n\nlinegraph\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nApr 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakfast Tableau - Multi-colored Waffle Charts and Pancakes\n\n\n\ntableau\n\n\nwafflechart\n\n\nbubblechart\n\n\nactions\n\n\nparameters\n\n\nimagebackgrounds\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\n\nJisell Howe\n\n\nApr 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJamming with Parameters and More in Tableau\n\n\n\ntableau\n\n\nparameters\n\n\nactions\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\n\nJisell Howe\n\n\nApr 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Stop Me Now - Animating Data Insights\n\n\n\ntableau\n\n\nanimation\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nMar 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSankey Charts - Creating a Fun Slide in Tableau Public\n\n\n\ntableau\n\n\nsankeychart\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\n\nJisell Howe\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWaffles Anyone? An Alternative to Pie Charts\n\n\n\ntableau\n\n\nwafflechart\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nFeb 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLicense to Simplify - Why Less is More in Data Viz\n\n\n\ntableau\n\n\nbarchart\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nFeb 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd Action! How to Invite the Viewer to Explore\n\n\n\ntableau\n\n\nactions\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLollipop Charts - A Sweet Data Viz Technique\n\n\n\ntableau\n\n\nlollipop\n\n\nshapes\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nJan 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJitterplots - An Easy Way to Make Your Insights Stand Out\n\n\n\ntableau\n\n\njitterplot\n\n\nanalysis\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nJan 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Quest for the Perfect BIM Project\n\n\n\nBIM\n\n\nconstruction\n\n\nmanufacturing\n\n\ncareer\n\n\n\n\n\n\n\nJisell Howe\n\n\nJan 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of Data Visualization in the AEC Industry - Pt. 1\n\n\n\nBIM\n\n\npowerbi\n\n\nconstruction\n\n\nmanufacturing\n\n\ntreemap\n\n\ncareer\n\n\n\n\n\n\n\nJisell Howe\n\n\nDec 27, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Benefits of Data Visualization in the AEC Industry - Pt. 2\n\n\n\nBIM\n\n\npowerbi\n\n\ntreemap\n\n\nbarchart\n\n\ncourses\n\n\nbooks\n\n\nmakeovermonday\n\n\n\n\n\n\n\nJisell Howe\n\n\nDec 27, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Résumé",
    "section": "",
    "text": "Manage BIM content creation projects for clients; New Product Development is my specialty\nProvide creative Marketing and Sales experiences via writing, website updates, social media, sketches, presentations, and visual strategy\nProvide BIM content demonstrations and webinars\nBuild No Code/Low Code/Code business integration and automation solutions"
  },
  {
    "objectID": "resume.html#augi-membership-manager",
    "href": "resume.html#augi-membership-manager",
    "title": "Résumé",
    "section": "AUGI Membership Manager",
    "text": "AUGI Membership Manager\n\nAUGI - January 2020 - Present\n\nSupport membership questions and forum name requests for AUGI, the world’s largest CAD & BIM user group"
  },
  {
    "objectID": "resume.html#cad-administrator",
    "href": "resume.html#cad-administrator",
    "title": "Résumé",
    "section": "CAD Administrator",
    "text": "CAD Administrator\n\nUponor North America - February 2014 - September 2020\n\nManaged and supported all software platforms, licensing/contracts, databases, templates, and CAD/BIM/PDM libraries for Construction Services. Platforms included Autodesk Revit, AutoCAD, Fabrication CADmep, SolidWorks, Bluebeam, and various software add-ons.\nLed BIM data related projects to coincide with manufacturer New Product Development for stakeholders on cross-functional teams. BIM owner.\nCreated promotional materials/software demos for Construction Services designers and external customers.\nEnhanced customer experience with technical support for customers downloading manufacturer BIM/CAD content from the website.\nCollaborated and built cross-functional relationships with internal stakeholders and various external vendors for complex projects, engineering change management, and BIM data process management. Extensive experience on remote collaboration and project management.\nUsed BI software such as Microsoft Power BI and Tableau for visualizing BIM & data management."
  },
  {
    "objectID": "resume.html#radiant-cad-designer",
    "href": "resume.html#radiant-cad-designer",
    "title": "Résumé",
    "section": "Radiant CAD Designer",
    "text": "Radiant CAD Designer\n\nUponor North America - January 2012 - February 2014\n(Contractor position until officially hired in October 2012)\n\nDesigned radiant heated flooring and snow/ice melting systems using MC4/AutoCAD and ADS software.\nCommunicated with customers/sales reps throughout all stages of the project.\nCompleted heat loss calculations and material lists based on project specifications.\nValue engineered hydronic components.\nPioneered the testing of new software for the radiant team specifically."
  },
  {
    "objectID": "resume.html#cad-designer",
    "href": "resume.html#cad-designer",
    "title": "Résumé",
    "section": "CAD Designer",
    "text": "CAD Designer\n\nAPi Construction Co. (APi GROUP) - July 2010 - December 2011\n\nUtilized AutoCAD to draw existing steel drawings and design industrial/lagging systems.\nRevised drawings, Bill of Materials, and sheet layouts according to industry standards.\nCreated three-dimensional models of industrial units in AutoCAD.\nPrepared details for project managers for RFIs and proposals\nPrepared status drawings and weld map counts for quality control\nAPi Week of Giving Advocate for United Way/Community Health Charities"
  },
  {
    "objectID": "speakertradingcards.html",
    "href": "speakertradingcards.html",
    "title": "Speaker Trading Cards",
    "section": "",
    "text": "Standard Business Cards are “Okay”\nIn November 2018, I got to thinking about standard business cards and how they really don’t put a name to a face. In my opinion, they are “okay,” but they don’t tell the whole story. Where is the creativity or the “wow” factor to really stand out? Where is the fun?\n\n\nLet’s Stand Out\nFrom that observation, I came up with an idea to use the idea of Baseball cards but feature conference speakers on them instead. I call them Speaker Trading Cards.\nSimilar to Baseball trading cards, the stats on the back of a Speaker Trading Card would actually be the number of times you’ve contributed to the industry, whether it is for writing, speaking, you name it. The possibilities are endless.\nYes, you read that correctly. The idea is that the cards feature you as a contributor, not just a specific selection of people. All are welcome to create these cards, and at the end of this page, you’ll find instructions on how to do exactly that.\nSpeakers aren’t the only ones who can create and use the cards; you could be a writer, a podcast host, and more. Additionally, you don’t even have to be in a specific industry in order to use them. How cool is that?\nI originally premiered them at Midwest University 2019. I then introduced the second iteration with a custom QR code at Autodesk University 2019.\nI hope to see you create your own cards, and I’m so excited to see what’s next!\nLet’s connect in a fun way and support each other as we put pen to paper and step on stage. For some, that might even be the first time! Mentorship matters, and these cards provide a great way to connect those who are new to the game to those who have been in the game for a while.\n\n\nReady to make your own?\nStep #1: Go to Topps.com/cardbuilder\nStep #2: Then, go to the Custom Cards section to get started\nStep #3: Next, add image, frame, team, text , and logo. In my case, I added a custom dynamic QR code as the “logo” from the-qrcode-generator.com/\nStep #4: Next, add first name, last name, and choose Custom Position for the front of the card. Then, specify your title or anything that suits you!\nStep #5: Next, design the back of the card with text and hometown. Add your professional development stats as well! Anything goes!\nStep #6: Finally, review and buy cards (20 cards – $9.99).\n\n\n\nThe original Speaker Trading Card launched at Midwest University 2019\n\n\n\n\n\nThis is the second (and latest) iteration of the Speaker Trading Card. It was launched at Autodesk University 2019."
  }
]